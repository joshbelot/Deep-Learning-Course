{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{},"source":["In this notebook we're going to build and train a deep learning model \"from scratch\" -- by which I mean that we're not going to use any pre-built architecture, or optimizers, or data loading frameworks, etc.\n","\n","We'll be assuming you already know the basics of how a neural network works. If you don't, read this notebook first: [How does a neural net really work?\n","](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work). We'll be using Kaggle's [Titanic](https://www.kaggle.com/competitions/titanic/) competition in this notebook, because it's very small and simple, but also has displays many of the tricky real-life issues that we need to handle in most practical projects. (Note, however, that this competition is a small \"learner\" competition on Kaggle, so don't expect to actually see much benefits from using a neural net just yet; that will come once we try our some real competitions!)\n","\n","It's great to be able to run the same notebook on your own machine or Colab, as well as Kaggle. To allow for this, we use this code to download the data as needed when not on Kaggle (see [this notebook](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners/) for details about this technique):"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:17.763822Z","iopub.status.busy":"2022-05-30T22:34:17.763494Z","iopub.status.idle":"2022-05-30T22:34:17.771348Z","shell.execute_reply":"2022-05-30T22:34:17.770444Z","shell.execute_reply.started":"2022-05-30T22:34:17.763787Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading digit-recognizer.zip to /Users/joshbelot/Documents/Deep Learning Course/Deep-Learning-Course/Kaggle Competitions/Digit Recognizer\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 15.3M/15.3M [00:04<00:00, 3.36MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["import os\n","from pathlib import Path\n","\n","iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n","if iskaggle: path = Path('../input/digit-recognizer')\n","else:\n","    path = Path('digit-recognizer')\n","    if not path.exists():\n","        import zipfile,kaggle\n","        kaggle.api.competition_download_cli(str(path))\n","        zipfile.ZipFile(f'{path}.zip').extractall(path)"]},{"cell_type":"markdown","metadata":{"hidden":true},"source":["Note that the data for Kaggle comps always lives in the `../input` folder. The easiest way to get the path is to click the \"K\" button in the top-right of the Kaggle notebook, click on the folder shown there, and click the copy button.\n","\n","We'll be using *numpy* and *pytorch* for array calculations in this notebook, and *pandas* for working with tabular data, so we'll import them and set them to display using a bit more space than they default to."]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:17.811857Z","iopub.status.busy":"2022-05-30T22:34:17.810967Z","iopub.status.idle":"2022-05-30T22:34:17.817725Z","shell.execute_reply":"2022-05-30T22:34:17.816849Z","shell.execute_reply.started":"2022-05-30T22:34:17.811797Z"},"trusted":true},"outputs":[],"source":["import torch, numpy as np, pandas as pd\n","np.set_printoptions(linewidth=140)\n","torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\n","pd.set_option('display.width', 140)"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true},"source":["## Cleaning the data"]},{"cell_type":"markdown","metadata":{"hidden":true},"source":["This is a *tabular data* competition -- the data is in the form of a table. It's provided as a Comma Separated Values (CSV) file. We can open it using the *pandas* library, which will create a `DataFrame`."]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:17.899238Z","iopub.status.busy":"2022-05-30T22:34:17.898249Z","iopub.status.idle":"2022-05-30T22:34:17.932714Z","shell.execute_reply":"2022-05-30T22:34:17.931738Z","shell.execute_reply.started":"2022-05-30T22:34:17.899131Z"},"hidden":true,"scrolled":true,"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>pixel0</th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>...</th>\n","      <th>pixel774</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>41995</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>41996</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>41997</th>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>41998</th>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>41999</th>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>42000 rows × 785 columns</p>\n","</div>"],"text/plain":["       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  ...  pixel774  pixel775  pixel776  pixel777  \\\n","0          1       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","1          0       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","2          1       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","3          4       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","4          0       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","...      ...     ...     ...     ...     ...     ...     ...     ...     ...     ...  ...       ...       ...       ...       ...   \n","41995      0       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","41996      1       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","41997      7       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","41998      6       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","41999      9       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","\n","       pixel778  pixel779  pixel780  pixel781  pixel782  pixel783  \n","0             0         0         0         0         0         0  \n","1             0         0         0         0         0         0  \n","2             0         0         0         0         0         0  \n","3             0         0         0         0         0         0  \n","4             0         0         0         0         0         0  \n","...         ...       ...       ...       ...       ...       ...  \n","41995         0         0         0         0         0         0  \n","41996         0         0         0         0         0         0  \n","41997         0         0         0         0         0         0  \n","41998         0         0         0         0         0         0  \n","41999         0         0         0         0         0         0  \n","\n","[42000 rows x 785 columns]"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(path/'train.csv')\n","df"]},{"cell_type":"markdown","metadata":{"hidden":true},"source":["Here's how we get a quick summary of all the numeric columns in the dataset:"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:18.205953Z","iopub.status.busy":"2022-05-30T22:34:18.205483Z","iopub.status.idle":"2022-05-30T22:34:18.240196Z","shell.execute_reply":"2022-05-30T22:34:18.239106Z","shell.execute_reply.started":"2022-05-30T22:34:18.205897Z"},"hidden":true,"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>pixel0</th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>...</th>\n","      <th>pixel774</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>42000.000000</td>\n","      <td>42000.0</td>\n","      <td>42000.0</td>\n","      <td>42000.0</td>\n","      <td>42000.0</td>\n","      <td>42000.0</td>\n","      <td>42000.0</td>\n","      <td>42000.0</td>\n","      <td>42000.0</td>\n","      <td>42000.0</td>\n","      <td>...</td>\n","      <td>42000.000000</td>\n","      <td>42000.000000</td>\n","      <td>42000.000000</td>\n","      <td>42000.00000</td>\n","      <td>42000.000000</td>\n","      <td>42000.000000</td>\n","      <td>42000.0</td>\n","      <td>42000.0</td>\n","      <td>42000.0</td>\n","      <td>42000.0</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>4.456643</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.219286</td>\n","      <td>0.117095</td>\n","      <td>0.059024</td>\n","      <td>0.02019</td>\n","      <td>0.017238</td>\n","      <td>0.002857</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>2.887730</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>6.312890</td>\n","      <td>4.633819</td>\n","      <td>3.274488</td>\n","      <td>1.75987</td>\n","      <td>1.894498</td>\n","      <td>0.414264</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>2.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>4.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>7.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>9.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>254.000000</td>\n","      <td>254.000000</td>\n","      <td>253.000000</td>\n","      <td>253.00000</td>\n","      <td>254.000000</td>\n","      <td>62.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 785 columns</p>\n","</div>"],"text/plain":["              label   pixel0   pixel1   pixel2   pixel3   pixel4   pixel5   pixel6   pixel7   pixel8  ...      pixel774      pixel775  \\\n","count  42000.000000  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0  ...  42000.000000  42000.000000   \n","mean       4.456643      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0  ...      0.219286      0.117095   \n","std        2.887730      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0  ...      6.312890      4.633819   \n","min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0  ...      0.000000      0.000000   \n","25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0  ...      0.000000      0.000000   \n","50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0  ...      0.000000      0.000000   \n","75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0  ...      0.000000      0.000000   \n","max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0  ...    254.000000    254.000000   \n","\n","           pixel776     pixel777      pixel778      pixel779  pixel780  pixel781  pixel782  pixel783  \n","count  42000.000000  42000.00000  42000.000000  42000.000000   42000.0   42000.0   42000.0   42000.0  \n","mean       0.059024      0.02019      0.017238      0.002857       0.0       0.0       0.0       0.0  \n","std        3.274488      1.75987      1.894498      0.414264       0.0       0.0       0.0       0.0  \n","min        0.000000      0.00000      0.000000      0.000000       0.0       0.0       0.0       0.0  \n","25%        0.000000      0.00000      0.000000      0.000000       0.0       0.0       0.0       0.0  \n","50%        0.000000      0.00000      0.000000      0.000000       0.0       0.0       0.0       0.0  \n","75%        0.000000      0.00000      0.000000      0.000000       0.0       0.0       0.0       0.0  \n","max      253.000000    253.00000    254.000000     62.000000       0.0       0.0       0.0       0.0  \n","\n","[8 rows x 785 columns]"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","\n","df.describe(include=(np.number))"]},{"cell_type":"markdown","metadata":{"hidden":true},"source":["Now we can create our independent (predictors) and dependent (target) variables. They both need to be PyTorch tensors. Our dependent variable is `label`:"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:18.935422Z","iopub.status.busy":"2022-05-30T22:34:18.934648Z","iopub.status.idle":"2022-05-30T22:34:18.944596Z","shell.execute_reply":"2022-05-30T22:34:18.943444Z","shell.execute_reply.started":"2022-05-30T22:34:18.935384Z"},"hidden":true,"trusted":true},"outputs":[{"data":{"text/plain":["tensor([1, 0, 1, 4, 0, 0, 7,  ..., 6, 4, 0, 1, 7, 6, 9])"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["from torch import tensor\n","\n","t_dep = tensor(df.label)\n","t_dep"]},{"cell_type":"markdown","metadata":{"hidden":true},"source":["Our independent variables are all the continuous variables of interest plus all the dummy variables we just created:"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pixel0</th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>pixel9</th>\n","      <th>...</th>\n","      <th>pixel774</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>41995</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>41996</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>41997</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>41998</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>41999</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>42000 rows × 784 columns</p>\n","</div>"],"text/plain":["       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  ...  pixel774  pixel775  pixel776  pixel777  \\\n","0           0       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","1           0       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","2           0       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","3           0       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","4           0       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","...       ...     ...     ...     ...     ...     ...     ...     ...     ...     ...  ...       ...       ...       ...       ...   \n","41995       0       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","41996       0       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","41997       0       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","41998       0       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","41999       0       0       0       0       0       0       0       0       0       0  ...         0         0         0         0   \n","\n","       pixel778  pixel779  pixel780  pixel781  pixel782  pixel783  \n","0             0         0         0         0         0         0  \n","1             0         0         0         0         0         0  \n","2             0         0         0         0         0         0  \n","3             0         0         0         0         0         0  \n","4             0         0         0         0         0         0  \n","...         ...       ...       ...       ...       ...       ...  \n","41995         0         0         0         0         0         0  \n","41996         0         0         0         0         0         0  \n","41997         0         0         0         0         0         0  \n","41998         0         0         0         0         0         0  \n","41999         0         0         0         0         0         0  \n","\n","[42000 rows x 784 columns]"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["df.drop('label',axis=1)"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:18.946772Z","iopub.status.busy":"2022-05-30T22:34:18.94652Z","iopub.status.idle":"2022-05-30T22:34:18.96487Z","shell.execute_reply":"2022-05-30T22:34:18.963667Z","shell.execute_reply.started":"2022-05-30T22:34:18.946741Z"},"hidden":true,"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0.]])"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["indep_vars = df.drop('label',axis=1)\n","t_indep = tensor(indep_vars.values, dtype=torch.float)\n","t_indep"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:18.969136Z","iopub.status.busy":"2022-05-30T22:34:18.968005Z","iopub.status.idle":"2022-05-30T22:34:18.98114Z","shell.execute_reply":"2022-05-30T22:34:18.980184Z","shell.execute_reply.started":"2022-05-30T22:34:18.969092Z"},"hidden":true,"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([42000, 784])"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["t_indep.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Setting up a linear model"]},{"cell_type":"markdown","metadata":{},"source":["Now that we've got a matrix of independent variables and a dependent variable vector, we can work on calculating our predictions and our loss. In this section, we're going to manually do a single step of calculating predictions and loss for every row of our data.\n","\n","Our first model will be a simple linear model. We'll need a coefficient for each column in `t_indep`. We'll pick random numbers in the range `(-0.5,0.5)`, and set our manual seed so that my explanations in the prose in this notebook will be consistent with what you see when you run it."]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:18.983237Z","iopub.status.busy":"2022-05-30T22:34:18.982492Z","iopub.status.idle":"2022-05-30T22:34:18.995038Z","shell.execute_reply":"2022-05-30T22:34:18.994437Z","shell.execute_reply.started":"2022-05-30T22:34:18.983187Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([    -0.4629,      0.1386,      0.2409,     -0.2262,     -0.2632,     -0.3147,      0.4876,      0.3136,      0.2799,     -0.4392,\n","             0.2103,      0.3625,      0.1722,      0.2324,     -0.3575,     -0.0010,     -0.1833,     -0.2411,      0.0489,      0.0866,\n","            -0.0534,      0.3132,     -0.1487,     -0.2551,      0.3328,      0.1292,      0.2361,     -0.2622,      0.4051,     -0.2674,\n","            -0.2312,      0.1147,      0.4072,      0.2834,      0.0835,     -0.0504,      0.4354,     -0.3408,     -0.4552,      0.1447,\n","            -0.4648,     -0.3039,     -0.1297,     -0.0850,      0.4682,      0.3973,     -0.4849,      0.2098,     -0.3018,      0.4185,\n","             0.1089,      0.0791,     -0.1708,     -0.1882,      0.4861,     -0.2534,     -0.2502,     -0.0770,      0.3336,     -0.4975,\n","            -0.2561,      0.0892,     -0.4670,      0.3725,     -0.1095,      0.3443,     -0.3448,      0.1112,     -0.2866,      0.0245,\n","            -0.1640,     -0.0845,     -0.1318,      0.3686,      0.3743,      0.4772,     -0.0329,     -0.3076,      0.1449,      0.3728,\n","             0.1066,     -0.1043,     -0.4297,     -0.0069,     -0.2685,      0.2132,     -0.0946,     -0.4802,      0.1940,      0.0919,\n","             0.2913,     -0.2694,      0.4454,     -0.2993,      0.1581,      0.3215,     -0.1481,      0.2920,      0.0677,      0.2740,\n","             0.2386,     -0.2730,     -0.3255,     -0.3695,      0.1019,     -0.2950,     -0.3885,     -0.3118,     -0.1861,      0.3498,\n","            -0.4068,     -0.3355,     -0.4195,      0.4183,      0.0745,     -0.2618,      0.1313,      0.1043,     -0.1724,      0.2857,\n","             0.1229,     -0.1264,     -0.3666,     -0.1924,      0.2897,     -0.3271,      0.0228,      0.1612,     -0.1394,      0.4221,\n","            -0.0308,     -0.2891,     -0.3974,      0.0643,      0.0065,      0.2851,      0.0101,      0.2865,     -0.2499,     -0.3194,\n","            -0.0428,      0.2932,     -0.1902,     -0.4145,     -0.1962,     -0.1351,     -0.1406,     -0.0908,     -0.3329,      0.0873,\n","            -0.2703,      0.1628,     -0.2271,      0.0397,     -0.1825,      0.0371,      0.3826,     -0.3378,     -0.1353,     -0.1325,\n","            -0.0484,      0.4183,     -0.3299,      0.4589,      0.4941,      0.0707,      0.4651,      0.3530,      0.0762,      0.1317,\n","             0.1661,      0.0818,     -0.4650,     -0.3994,     -0.3703,     -0.0912,     -0.2068,      0.2324,     -0.4656,     -0.4877,\n","             0.3365,     -0.3951,      0.2695,      0.4371,     -0.0985,     -0.2637,     -0.0712,     -0.4714,      0.3475,     -0.2116,\n","            -0.0679,     -0.2907,     -0.2086,      0.4594,     -0.1492,     -0.3911,      0.3088,     -0.0719,     -0.4050,      0.3429,\n","             0.1524,      0.0166,     -0.4796,      0.2852,      0.0615,      0.0503,     -0.2562,      0.0253,     -0.3938,      0.3821,\n","             0.4183,      0.2715,     -0.2130,     -0.4010,     -0.2978,      0.4096,     -0.0851,     -0.4942,      0.3208,     -0.4139,\n","            -0.4086,      0.2151,     -0.4306,     -0.4858,      0.0428,     -0.0674,     -0.1172,      0.2743,     -0.4395,     -0.2553,\n","             0.2083,      0.3248,     -0.4198,     -0.3337,     -0.4108,      0.1121,      0.3272,     -0.3012,      0.4801,      0.0653,\n","             0.3900,      0.0846,     -0.1057,     -0.3032,     -0.4705,     -0.1387,      0.1520,      0.0367,      0.3844,      0.0475,\n","            -0.1029,     -0.3455,     -0.2749,      0.1893,     -0.3460,     -0.2611,      0.4122,      0.4237,      0.3237,     -0.4802,\n","             0.2368,     -0.3713,      0.4065,     -0.4607,     -0.0667,      0.1749,      0.1841,      0.1731,      0.0954,     -0.4919,\n","            -0.0406,     -0.2638,      0.4604,      0.3294,      0.2893,      0.4933,     -0.3774,      0.3779,     -0.1323,      0.0469,\n","             0.0855,      0.2162,     -0.2465,     -0.4605,      0.3043,      0.4040,     -0.0273,     -0.4982,     -0.0243,      0.3685,\n","            -0.0251,      0.4414,     -0.2516,      0.3610,     -0.3826,     -0.3601,     -0.4411,     -0.3187,      0.2338,      0.4351,\n","            -0.0826,     -0.2543,      0.4476,      0.1756,     -0.1745,     -0.1051,      0.0475,     -0.2269,     -0.3954,     -0.1494,\n","             0.3878,     -0.4689,      0.2499,     -0.1572,     -0.3014,     -0.0832,      0.1529,     -0.1350,     -0.3500,      0.1458,\n","             0.0670,     -0.4335,     -0.4428,      0.0666,     -0.0667,     -0.3216,     -0.3284,      0.3312,      0.4469,     -0.4205,\n","            -0.2452,     -0.1290,      0.1254,      0.4019,      0.4514,     -0.3047,      0.2062,     -0.3032,     -0.2616,      0.3936,\n","            -0.3898,     -0.1344,     -0.0367,      0.4341,      0.2416,      0.1916,     -0.0473,     -0.1164,     -0.1787,      0.0754,\n","            -0.4979,      0.1829,      0.2222,      0.3079,     -0.4185,     -0.0227,      0.1641,      0.1503,     -0.3057,      0.3884,\n","            -0.4534,     -0.3880,     -0.0919,     -0.2869,      0.2481,     -0.2725,     -0.1479,     -0.0968,      0.4744,      0.3729,\n","            -0.2586,     -0.2232,      0.0979,      0.3332,      0.2641,      0.4864,     -0.1363,      0.0143,     -0.4148,     -0.0749,\n","             0.3676,     -0.1419,     -0.4687,     -0.0561,     -0.0998,     -0.4347,      0.1905,     -0.0521,      0.4208,      0.0887,\n","            -0.3157,     -0.4251,     -0.1012,     -0.0954,     -0.1277,      0.3092,      0.4935,      0.0359,      0.1959,      0.2771,\n","            -0.3732,     -0.1157,     -0.0292,      0.4593,      0.3998,     -0.2353,      0.0389,     -0.4340,      0.1479,      0.1742,\n","             0.0245,      0.1730,      0.1545,      0.1229,      0.3963,     -0.4399,     -0.2960,      0.1244,      0.1591,      0.3703,\n","             0.0802,      0.1926,     -0.2711,     -0.4392,      0.0749,     -0.4605,     -0.2151,     -0.0271,      0.3788,     -0.2418,\n","             0.0554,      0.0589,     -0.1937,      0.4323,     -0.2884,     -0.4610,      0.2956,      0.0084,     -0.2322,      0.3884,\n","             0.4422,      0.3324,      0.2301,     -0.4755,      0.0666,      0.4014,     -0.3917,      0.4525,     -0.0623,     -0.4601,\n","            -0.3949,      0.1045,     -0.1289,     -0.0092,     -0.1064,     -0.1755,     -0.1107,      0.0851,     -0.0725,     -0.1673,\n","            -0.2583,     -0.1018,     -0.2167,      0.1052,     -0.4267,      0.1148,     -0.0874,     -0.3063,      0.4799,      0.3941,\n","             0.2568,     -0.0473,      0.0348,     -0.0604,      0.0872,      0.1258,      0.0892,      0.1236,     -0.4509,     -0.2376,\n","             0.4571,      0.0325,     -0.2479,      0.2197,     -0.0311,      0.0509,     -0.2659,     -0.1934,      0.0816,     -0.0781,\n","             0.1631,      0.2044,      0.4915,      0.0969,     -0.1580,      0.3973,     -0.1773,     -0.4408,     -0.4782,      0.4462,\n","             0.4737,      0.0182,     -0.2608,     -0.3492,     -0.2073,      0.2667,     -0.2695,     -0.0201,     -0.3172,     -0.1612,\n","             0.0410,     -0.0215,      0.2071,      0.0426,     -0.3506,     -0.4389,     -0.2437,     -0.4112,      0.0262,      0.3860,\n","            -0.1578,      0.4260,      0.1472,      0.0180,     -0.4649,      0.1349,     -0.2483,      0.1226,     -0.2593,     -0.3880,\n","             0.1882,      0.2329,     -0.4890,      0.1850,      0.4495,      0.0329,      0.1869,     -0.0147,     -0.3896,     -0.2754,\n","             0.2505,      0.1883,     -0.1156,     -0.1244,     -0.0322,     -0.2357,      0.4769,     -0.0315,      0.1088,      0.4334,\n","            -0.3965,      0.3998,      0.0198,      0.1732,      0.2181,     -0.1986,     -0.0062,     -0.2270,      0.1294,     -0.4437,\n","            -0.1012,      0.0240,     -0.4151,      0.3806,     -0.3297,      0.3646,     -0.4082,      0.3202,      0.4626,     -0.0101,\n","             0.3897,      0.4191,      0.2752,      0.1863,      0.4535,     -0.4850,      0.0335,      0.4918,      0.3873,     -0.4344,\n","             0.2389,     -0.0583,      0.4529,      0.4412,      0.0322,      0.1892,     -0.2216,      0.3127,     -0.3777,      0.3265,\n","             0.2219,     -0.3271,     -0.2389,     -0.4902,     -0.3468,     -0.4127,     -0.4422,     -0.3229,      0.3428,     -0.2132,\n","             0.2762,     -0.1323,      0.1963,      0.0186,     -0.0612,      0.4797,      0.0948,      0.0533,     -0.0749,     -0.1364,\n","            -0.2449,     -0.2234,     -0.1402,     -0.2344,      0.0794,      0.2478,     -0.4342,      0.0343,     -0.0662,     -0.0003,\n","            -0.3780,      0.0958,      0.0712,     -0.0827,     -0.0692,      0.0182,     -0.2397,      0.3964,      0.0133,     -0.3443,\n","             0.3146,     -0.1274,      0.2665,      0.0740,     -0.2221,      0.2590,     -0.3530,     -0.4124,     -0.0807,     -0.3178,\n","            -0.0074,     -0.3072,      0.4172,     -0.3776,     -0.1271,      0.1943,     -0.4005,      0.2562,      0.4476,      0.4599,\n","            -0.4058,     -0.3055,      0.3784,      0.0607,      0.1908,      0.2974,      0.2433,      0.1701,      0.1158,      0.3205,\n","            -0.4489,      0.1055,      0.1362,     -0.2316,      0.0961,     -0.1336,     -0.1018,     -0.3876,     -0.4665,     -0.1725,\n","             0.3461,      0.0519,     -0.3962,      0.0602,     -0.1812,     -0.0732,     -0.3765,      0.2039,      0.3407,     -0.3835,\n","            -0.2684,     -0.2702,     -0.0505,      0.1526,      0.4598,      0.0539,      0.4686,     -0.1660,      0.1872,     -0.3970,\n","             0.2495,      0.4257,     -0.1939,     -0.2056,      0.1595,     -0.2877,     -0.0060,     -0.2125,     -0.3806,     -0.2650,\n","             0.2225,     -0.3797,      0.1350,      0.2076,      0.3319,      0.3423,      0.2095,     -0.4377,     -0.2333,      0.0012,\n","             0.4149,     -0.3459,      0.1414,     -0.3875,      0.4889,     -0.4837,      0.1685,      0.2737,     -0.1566,     -0.2267,\n","            -0.2444,     -0.0052,     -0.4613,      0.3863,      0.1034,     -0.0864,      0.0629,     -0.4212,     -0.3595,     -0.0070,\n","            -0.2864,      0.0273,      0.2372,     -0.0308,      0.0415,      0.2878,      0.3158,     -0.4074,      0.1940,     -0.1409,\n","            -0.2848,     -0.3087,      0.2640,     -0.0683,     -0.4312,      0.4943,      0.3714,      0.3358,      0.3047,     -0.4884,\n","            -0.4849,      0.2262,      0.2172,      0.0323,     -0.0769,     -0.1767,     -0.1453,      0.3159,     -0.0257,      0.1365,\n","            -0.1752,     -0.0454,      0.1940,      0.2265,      0.3783,      0.2533,      0.0709,     -0.2586,      0.1527,      0.0794,\n","             0.4430,      0.1529,     -0.0938,     -0.3296,      0.3006,      0.4696,      0.1227,     -0.0055,     -0.2684,     -0.4286,\n","            -0.1200,      0.0068,      0.3211,     -0.3081])"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(442)\n","\n","n_coeff = t_indep.shape[1]\n","coeffs = torch.rand(n_coeff)-0.5\n","coeffs"]},{"cell_type":"markdown","metadata":{},"source":["Our predictions will be calculated by multiplying each row by the coefficients, and adding them up. One interesting point here is that we don't need a separate constant term (also known as a \"bias\" or \"intercept\" term), or a column of all `1`s to give the same effect has having a constant term. That's because our dummy variables already cover the entire dataset -- e.g. there's a column for \"male\" and a column for \"female\", and everyone in the dataset is in exactly one of these; therefore, we don't need a separate intercept term to cover rows that aren't otherwise part of a column.\n","\n","Here's what the multiplication looks like:"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:19.000637Z","iopub.status.busy":"2022-05-30T22:34:18.999409Z","iopub.status.idle":"2022-05-30T22:34:19.009362Z","shell.execute_reply":"2022-05-30T22:34:19.00847Z","shell.execute_reply.started":"2022-05-30T22:34:19.000588Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        ...,\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.]])"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["t_indep*coeffs"]},{"cell_type":"markdown","metadata":{},"source":["We can see we've got a problem here. The sums of each row will be dominated by the first column, which is `Age`, since that's bigger on average than all the others.\n","\n","Let's make all the columns contain numbers from `0` to `1`, by dividing each column by its `max()`:"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:19.011202Z","iopub.status.busy":"2022-05-30T22:34:19.010954Z","iopub.status.idle":"2022-05-30T22:34:19.02202Z","shell.execute_reply":"2022-05-30T22:34:19.02133Z","shell.execute_reply.started":"2022-05-30T22:34:19.011171Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 116., 254., 216.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  16.,  47., 157., 254., 255., 243., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 244., 255., 184., 197.,   0.,   0.,   0.,   0.,   0.,   0.,  64.,  29., 134., 128., 234., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 165.,   0.,   0.,   0.,   0., 141.,  84.,\n","        139., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 164.,\n","        121.,   0.,   0.,  38.,  51., 114., 226., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 254., 230.,   0.,   0.,   0.,  95., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 253.,  18.,   0.,   4., 177., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 254., 253.,   0., 128.,\n","        254., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 196.,  53., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 254., 190., 184., 254., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 254., 220., 226., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 254., 243.,\n","        150., 254., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 253., 112., 163., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 252., 110.,  32., 253., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 252., 247.,   0., 188., 254., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        209.,  51.,   0.,   0., 254., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 190.,   0.,  47., 254., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 254., 223.,   0.,  71., 254., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 253., 121.,  60., 126.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 151.,   0., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 253., 128.,   0., 178., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,  50.,  32., 107., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 251.,  39.,\n","         31.,  10., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 253., 225.,  72.,   0.,   0., 217., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 241., 150.,   0.,   0.,   0., 253., 253., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 254., 241.,  98.,   0.,   0.,   0.,  42., 254.,\n","        255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 196., 127.,\n","        104.,   0.,   0.,   0.,   0.,   0., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255., 255.,\n","        255., 255., 255., 253.,  28.,  59.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 177., 231., 253., 254., 254., 255., 255., 255., 255.,\n","        255., 255., 255., 255., 254., 254., 253., 253., 254.,  62.,   0.,   0.,   0.,   0.])\n"]}],"source":["vals,indices = t_indep.max(dim=0)\n","print(vals)\n","# Avoid division by zero by setting zero max values to one (or some other non-zero value)\n","safe_vals = torch.where(vals == 0.0, torch.tensor(1.0), vals)\n","t_indep = t_indep / safe_vals"]},{"cell_type":"markdown","metadata":{},"source":["As we see, that removes the problem of one column dominating all the others:"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:19.04269Z","iopub.status.busy":"2022-05-30T22:34:19.042223Z","iopub.status.idle":"2022-05-30T22:34:19.050475Z","shell.execute_reply":"2022-05-30T22:34:19.049515Z","shell.execute_reply.started":"2022-05-30T22:34:19.042652Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        ...,\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.],\n","        [-0., 0., 0., -0., -0., -0., 0.,  ..., -0., -0., -0., -0., 0., 0., -0.]])"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["t_indep*coeffs"]},{"cell_type":"markdown","metadata":{},"source":["One thing you hopefully noticed is how amazingly cool this line of code is:\n","\n","    t_indep = t_indep / vals\n","\n","That is dividing a matrix by a vector -- what on earth does that mean?!? The trick here is that we're taking advantage of a technique in numpy and PyTorch (and many other languages, going all the way back to APL) called [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html). In short, this acts as if there's a separate copy of the vector for every row of the matrix, so it divides each row of the matrix by the vector. In practice, it doesn't actually make any copies, and does the whole thing in a highly optimized way, taking full advantage of modern CPUs (or, indeed, GPUs, if we're using them). Broadcasting is one of the most important techniques for making your code concise, maintainable, and fast, so it's well worth studying and practicing.\n","\n","We can now create predictions from our linear model, by adding up the rows of the product:"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:19.097604Z","iopub.status.busy":"2022-05-30T22:34:19.096685Z","iopub.status.idle":"2022-05-30T22:34:19.1028Z","shell.execute_reply":"2022-05-30T22:34:19.10218Z","shell.execute_reply.started":"2022-05-30T22:34:19.097545Z"},"trusted":true},"outputs":[],"source":["preds = (t_indep*coeffs).sum(axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["Let's take a look at the first few:"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:19.160794Z","iopub.status.busy":"2022-05-30T22:34:19.160318Z","iopub.status.idle":"2022-05-30T22:34:19.168117Z","shell.execute_reply":"2022-05-30T22:34:19.167355Z","shell.execute_reply.started":"2022-05-30T22:34:19.160761Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([-4.7737,  3.3516, -0.1378, -1.7691,  0.5001,  0.3763,  0.1946, -0.7404,  2.9025, -0.8442])"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["preds[:10]"]},{"cell_type":"markdown","metadata":{},"source":["Of course, these predictions aren't going to be any use, since our coefficients are random -- they're just a starting point for our gradient descent process.\n","\n","To do gradient descent, we need a loss function. Taking the average error of the rows (i.e. the absolute value of the difference between the prediction and the dependent) is generally a reasonable approach:"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:19.235623Z","iopub.status.busy":"2022-05-30T22:34:19.235318Z","iopub.status.idle":"2022-05-30T22:34:19.24312Z","shell.execute_reply":"2022-05-30T22:34:19.242356Z","shell.execute_reply.started":"2022-05-30T22:34:19.235589Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor(4.3207)"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["loss = torch.abs(preds-t_dep).mean()\n","loss"]},{"cell_type":"markdown","metadata":{},"source":["Now that we've tested out a way of calculating predictions, and loss, let's pop them into functions to make life easier:"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:19.296558Z","iopub.status.busy":"2022-05-30T22:34:19.295577Z","iopub.status.idle":"2022-05-30T22:34:19.301478Z","shell.execute_reply":"2022-05-30T22:34:19.300683Z","shell.execute_reply.started":"2022-05-30T22:34:19.296517Z"},"trusted":true},"outputs":[],"source":["def calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)\n","def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()"]},{"cell_type":"markdown","metadata":{},"source":["## Doing a gradient descent step"]},{"cell_type":"markdown","metadata":{},"source":["In this section, we're going to do a single \"epoch\" of gradient descent manually. The only thing we're going to automate is calculating gradients, because let's face it that's pretty tedious and entirely pointless to do by hand! To get PyTorch to calculate gradients, we'll need to call `requires_grad_()` on our `coeffs` (if you're not sure why, review the previous notebook, [How does a neural net really work?](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work), before continuing):"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:19.376212Z","iopub.status.busy":"2022-05-30T22:34:19.375387Z","iopub.status.idle":"2022-05-30T22:34:19.382205Z","shell.execute_reply":"2022-05-30T22:34:19.381536Z","shell.execute_reply.started":"2022-05-30T22:34:19.376163Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([    -0.4629,      0.1386,      0.2409,     -0.2262,     -0.2632,     -0.3147,      0.4876,      0.3136,      0.2799,     -0.4392,\n","             0.2103,      0.3625,      0.1722,      0.2324,     -0.3575,     -0.0010,     -0.1833,     -0.2411,      0.0489,      0.0866,\n","            -0.0534,      0.3132,     -0.1487,     -0.2551,      0.3328,      0.1292,      0.2361,     -0.2622,      0.4051,     -0.2674,\n","            -0.2312,      0.1147,      0.4072,      0.2834,      0.0835,     -0.0504,      0.4354,     -0.3408,     -0.4552,      0.1447,\n","            -0.4648,     -0.3039,     -0.1297,     -0.0850,      0.4682,      0.3973,     -0.4849,      0.2098,     -0.3018,      0.4185,\n","             0.1089,      0.0791,     -0.1708,     -0.1882,      0.4861,     -0.2534,     -0.2502,     -0.0770,      0.3336,     -0.4975,\n","            -0.2561,      0.0892,     -0.4670,      0.3725,     -0.1095,      0.3443,     -0.3448,      0.1112,     -0.2866,      0.0245,\n","            -0.1640,     -0.0845,     -0.1318,      0.3686,      0.3743,      0.4772,     -0.0329,     -0.3076,      0.1449,      0.3728,\n","             0.1066,     -0.1043,     -0.4297,     -0.0069,     -0.2685,      0.2132,     -0.0946,     -0.4802,      0.1940,      0.0919,\n","             0.2913,     -0.2694,      0.4454,     -0.2993,      0.1581,      0.3215,     -0.1481,      0.2920,      0.0677,      0.2740,\n","             0.2386,     -0.2730,     -0.3255,     -0.3695,      0.1019,     -0.2950,     -0.3885,     -0.3118,     -0.1861,      0.3498,\n","            -0.4068,     -0.3355,     -0.4195,      0.4183,      0.0745,     -0.2618,      0.1313,      0.1043,     -0.1724,      0.2857,\n","             0.1229,     -0.1264,     -0.3666,     -0.1924,      0.2897,     -0.3271,      0.0228,      0.1612,     -0.1394,      0.4221,\n","            -0.0308,     -0.2891,     -0.3974,      0.0643,      0.0065,      0.2851,      0.0101,      0.2865,     -0.2499,     -0.3194,\n","            -0.0428,      0.2932,     -0.1902,     -0.4145,     -0.1962,     -0.1351,     -0.1406,     -0.0908,     -0.3329,      0.0873,\n","            -0.2703,      0.1628,     -0.2271,      0.0397,     -0.1825,      0.0371,      0.3826,     -0.3378,     -0.1353,     -0.1325,\n","            -0.0484,      0.4183,     -0.3299,      0.4589,      0.4941,      0.0707,      0.4651,      0.3530,      0.0762,      0.1317,\n","             0.1661,      0.0818,     -0.4650,     -0.3994,     -0.3703,     -0.0912,     -0.2068,      0.2324,     -0.4656,     -0.4877,\n","             0.3365,     -0.3951,      0.2695,      0.4371,     -0.0985,     -0.2637,     -0.0712,     -0.4714,      0.3475,     -0.2116,\n","            -0.0679,     -0.2907,     -0.2086,      0.4594,     -0.1492,     -0.3911,      0.3088,     -0.0719,     -0.4050,      0.3429,\n","             0.1524,      0.0166,     -0.4796,      0.2852,      0.0615,      0.0503,     -0.2562,      0.0253,     -0.3938,      0.3821,\n","             0.4183,      0.2715,     -0.2130,     -0.4010,     -0.2978,      0.4096,     -0.0851,     -0.4942,      0.3208,     -0.4139,\n","            -0.4086,      0.2151,     -0.4306,     -0.4858,      0.0428,     -0.0674,     -0.1172,      0.2743,     -0.4395,     -0.2553,\n","             0.2083,      0.3248,     -0.4198,     -0.3337,     -0.4108,      0.1121,      0.3272,     -0.3012,      0.4801,      0.0653,\n","             0.3900,      0.0846,     -0.1057,     -0.3032,     -0.4705,     -0.1387,      0.1520,      0.0367,      0.3844,      0.0475,\n","            -0.1029,     -0.3455,     -0.2749,      0.1893,     -0.3460,     -0.2611,      0.4122,      0.4237,      0.3237,     -0.4802,\n","             0.2368,     -0.3713,      0.4065,     -0.4607,     -0.0667,      0.1749,      0.1841,      0.1731,      0.0954,     -0.4919,\n","            -0.0406,     -0.2638,      0.4604,      0.3294,      0.2893,      0.4933,     -0.3774,      0.3779,     -0.1323,      0.0469,\n","             0.0855,      0.2162,     -0.2465,     -0.4605,      0.3043,      0.4040,     -0.0273,     -0.4982,     -0.0243,      0.3685,\n","            -0.0251,      0.4414,     -0.2516,      0.3610,     -0.3826,     -0.3601,     -0.4411,     -0.3187,      0.2338,      0.4351,\n","            -0.0826,     -0.2543,      0.4476,      0.1756,     -0.1745,     -0.1051,      0.0475,     -0.2269,     -0.3954,     -0.1494,\n","             0.3878,     -0.4689,      0.2499,     -0.1572,     -0.3014,     -0.0832,      0.1529,     -0.1350,     -0.3500,      0.1458,\n","             0.0670,     -0.4335,     -0.4428,      0.0666,     -0.0667,     -0.3216,     -0.3284,      0.3312,      0.4469,     -0.4205,\n","            -0.2452,     -0.1290,      0.1254,      0.4019,      0.4514,     -0.3047,      0.2062,     -0.3032,     -0.2616,      0.3936,\n","            -0.3898,     -0.1344,     -0.0367,      0.4341,      0.2416,      0.1916,     -0.0473,     -0.1164,     -0.1787,      0.0754,\n","            -0.4979,      0.1829,      0.2222,      0.3079,     -0.4185,     -0.0227,      0.1641,      0.1503,     -0.3057,      0.3884,\n","            -0.4534,     -0.3880,     -0.0919,     -0.2869,      0.2481,     -0.2725,     -0.1479,     -0.0968,      0.4744,      0.3729,\n","            -0.2586,     -0.2232,      0.0979,      0.3332,      0.2641,      0.4864,     -0.1363,      0.0143,     -0.4148,     -0.0749,\n","             0.3676,     -0.1419,     -0.4687,     -0.0561,     -0.0998,     -0.4347,      0.1905,     -0.0521,      0.4208,      0.0887,\n","            -0.3157,     -0.4251,     -0.1012,     -0.0954,     -0.1277,      0.3092,      0.4935,      0.0359,      0.1959,      0.2771,\n","            -0.3732,     -0.1157,     -0.0292,      0.4593,      0.3998,     -0.2353,      0.0389,     -0.4340,      0.1479,      0.1742,\n","             0.0245,      0.1730,      0.1545,      0.1229,      0.3963,     -0.4399,     -0.2960,      0.1244,      0.1591,      0.3703,\n","             0.0802,      0.1926,     -0.2711,     -0.4392,      0.0749,     -0.4605,     -0.2151,     -0.0271,      0.3788,     -0.2418,\n","             0.0554,      0.0589,     -0.1937,      0.4323,     -0.2884,     -0.4610,      0.2956,      0.0084,     -0.2322,      0.3884,\n","             0.4422,      0.3324,      0.2301,     -0.4755,      0.0666,      0.4014,     -0.3917,      0.4525,     -0.0623,     -0.4601,\n","            -0.3949,      0.1045,     -0.1289,     -0.0092,     -0.1064,     -0.1755,     -0.1107,      0.0851,     -0.0725,     -0.1673,\n","            -0.2583,     -0.1018,     -0.2167,      0.1052,     -0.4267,      0.1148,     -0.0874,     -0.3063,      0.4799,      0.3941,\n","             0.2568,     -0.0473,      0.0348,     -0.0604,      0.0872,      0.1258,      0.0892,      0.1236,     -0.4509,     -0.2376,\n","             0.4571,      0.0325,     -0.2479,      0.2197,     -0.0311,      0.0509,     -0.2659,     -0.1934,      0.0816,     -0.0781,\n","             0.1631,      0.2044,      0.4915,      0.0969,     -0.1580,      0.3973,     -0.1773,     -0.4408,     -0.4782,      0.4462,\n","             0.4737,      0.0182,     -0.2608,     -0.3492,     -0.2073,      0.2667,     -0.2695,     -0.0201,     -0.3172,     -0.1612,\n","             0.0410,     -0.0215,      0.2071,      0.0426,     -0.3506,     -0.4389,     -0.2437,     -0.4112,      0.0262,      0.3860,\n","            -0.1578,      0.4260,      0.1472,      0.0180,     -0.4649,      0.1349,     -0.2483,      0.1226,     -0.2593,     -0.3880,\n","             0.1882,      0.2329,     -0.4890,      0.1850,      0.4495,      0.0329,      0.1869,     -0.0147,     -0.3896,     -0.2754,\n","             0.2505,      0.1883,     -0.1156,     -0.1244,     -0.0322,     -0.2357,      0.4769,     -0.0315,      0.1088,      0.4334,\n","            -0.3965,      0.3998,      0.0198,      0.1732,      0.2181,     -0.1986,     -0.0062,     -0.2270,      0.1294,     -0.4437,\n","            -0.1012,      0.0240,     -0.4151,      0.3806,     -0.3297,      0.3646,     -0.4082,      0.3202,      0.4626,     -0.0101,\n","             0.3897,      0.4191,      0.2752,      0.1863,      0.4535,     -0.4850,      0.0335,      0.4918,      0.3873,     -0.4344,\n","             0.2389,     -0.0583,      0.4529,      0.4412,      0.0322,      0.1892,     -0.2216,      0.3127,     -0.3777,      0.3265,\n","             0.2219,     -0.3271,     -0.2389,     -0.4902,     -0.3468,     -0.4127,     -0.4422,     -0.3229,      0.3428,     -0.2132,\n","             0.2762,     -0.1323,      0.1963,      0.0186,     -0.0612,      0.4797,      0.0948,      0.0533,     -0.0749,     -0.1364,\n","            -0.2449,     -0.2234,     -0.1402,     -0.2344,      0.0794,      0.2478,     -0.4342,      0.0343,     -0.0662,     -0.0003,\n","            -0.3780,      0.0958,      0.0712,     -0.0827,     -0.0692,      0.0182,     -0.2397,      0.3964,      0.0133,     -0.3443,\n","             0.3146,     -0.1274,      0.2665,      0.0740,     -0.2221,      0.2590,     -0.3530,     -0.4124,     -0.0807,     -0.3178,\n","            -0.0074,     -0.3072,      0.4172,     -0.3776,     -0.1271,      0.1943,     -0.4005,      0.2562,      0.4476,      0.4599,\n","            -0.4058,     -0.3055,      0.3784,      0.0607,      0.1908,      0.2974,      0.2433,      0.1701,      0.1158,      0.3205,\n","            -0.4489,      0.1055,      0.1362,     -0.2316,      0.0961,     -0.1336,     -0.1018,     -0.3876,     -0.4665,     -0.1725,\n","             0.3461,      0.0519,     -0.3962,      0.0602,     -0.1812,     -0.0732,     -0.3765,      0.2039,      0.3407,     -0.3835,\n","            -0.2684,     -0.2702,     -0.0505,      0.1526,      0.4598,      0.0539,      0.4686,     -0.1660,      0.1872,     -0.3970,\n","             0.2495,      0.4257,     -0.1939,     -0.2056,      0.1595,     -0.2877,     -0.0060,     -0.2125,     -0.3806,     -0.2650,\n","             0.2225,     -0.3797,      0.1350,      0.2076,      0.3319,      0.3423,      0.2095,     -0.4377,     -0.2333,      0.0012,\n","             0.4149,     -0.3459,      0.1414,     -0.3875,      0.4889,     -0.4837,      0.1685,      0.2737,     -0.1566,     -0.2267,\n","            -0.2444,     -0.0052,     -0.4613,      0.3863,      0.1034,     -0.0864,      0.0629,     -0.4212,     -0.3595,     -0.0070,\n","            -0.2864,      0.0273,      0.2372,     -0.0308,      0.0415,      0.2878,      0.3158,     -0.4074,      0.1940,     -0.1409,\n","            -0.2848,     -0.3087,      0.2640,     -0.0683,     -0.4312,      0.4943,      0.3714,      0.3358,      0.3047,     -0.4884,\n","            -0.4849,      0.2262,      0.2172,      0.0323,     -0.0769,     -0.1767,     -0.1453,      0.3159,     -0.0257,      0.1365,\n","            -0.1752,     -0.0454,      0.1940,      0.2265,      0.3783,      0.2533,      0.0709,     -0.2586,      0.1527,      0.0794,\n","             0.4430,      0.1529,     -0.0938,     -0.3296,      0.3006,      0.4696,      0.1227,     -0.0055,     -0.2684,     -0.4286,\n","            -0.1200,      0.0068,      0.3211,     -0.3081], requires_grad=True)"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["coeffs.requires_grad_()"]},{"cell_type":"markdown","metadata":{},"source":["Now when we calculate our loss, PyTorch will keep track of all the steps, so we'll be able to get the gradients afterwards:"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:19.438026Z","iopub.status.busy":"2022-05-30T22:34:19.437208Z","iopub.status.idle":"2022-05-30T22:34:19.444641Z","shell.execute_reply":"2022-05-30T22:34:19.443791Z","shell.execute_reply.started":"2022-05-30T22:34:19.437985Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor(4.3207, grad_fn=<MeanBackward0>)"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["loss = calc_loss(coeffs, t_indep, t_dep)\n","loss"]},{"cell_type":"markdown","metadata":{},"source":["Use `backward()` to ask PyTorch to calculate gradients now:"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:19.49239Z","iopub.status.busy":"2022-05-30T22:34:19.491601Z","iopub.status.idle":"2022-05-30T22:34:19.496556Z","shell.execute_reply":"2022-05-30T22:34:19.495831Z","shell.execute_reply.started":"2022-05-30T22:34:19.492353Z"},"trusted":true},"outputs":[],"source":["loss.backward()"]},{"cell_type":"markdown","metadata":{},"source":["Let's see what they look like:"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:19.550501Z","iopub.status.busy":"2022-05-30T22:34:19.549651Z","iopub.status.idle":"2022-05-30T22:34:19.555905Z","shell.execute_reply":"2022-05-30T22:34:19.555257Z","shell.execute_reply.started":"2022-05-30T22:34:19.550456Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n","             0.0000,      0.0000,     -0.0000,     -0.0000,     -0.0000,     -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n","             0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n","             0.0000,      0.0000,     -0.0000,     -0.0000,     -0.0001,     -0.0001,     -0.0002,     -0.0003,     -0.0005,     -0.0007,\n","            -0.0008,     -0.0007,     -0.0007,     -0.0006,     -0.0005,     -0.0005,     -0.0004,     -0.0002,     -0.0002,     -0.0001,\n","            -0.0001,     -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,      0.0000,\n","            -0.0001,     -0.0000,     -0.0002,     -0.0005,     -0.0010,     -0.0019,     -0.0031,     -0.0049,     -0.0071,     -0.0096,\n","            -0.0121,     -0.0130,     -0.0113,     -0.0091,     -0.0075,     -0.0065,     -0.0044,     -0.0024,     -0.0012,     -0.0004,\n","            -0.0001,     -0.0001,      0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,     -0.0000,     -0.0001,     -0.0003,\n","            -0.0008,     -0.0017,     -0.0038,     -0.0072,     -0.0120,     -0.0179,     -0.0246,     -0.0322,     -0.0387,     -0.0414,\n","            -0.0387,     -0.0351,     -0.0300,     -0.0230,     -0.0146,     -0.0082,     -0.0039,     -0.0015,     -0.0006,     -0.0001,\n","            -0.0000,      0.0000,      0.0000,     -0.0000,     -0.0001,     -0.0001,     -0.0003,     -0.0013,     -0.0035,     -0.0084,\n","            -0.0167,     -0.0282,     -0.0440,     -0.0623,     -0.0818,     -0.0993,     -0.1104,     -0.1133,     -0.1089,     -0.0970,\n","            -0.0791,     -0.0607,     -0.0416,     -0.0260,     -0.0145,     -0.0072,     -0.0029,     -0.0007,     -0.0001,      0.0000,\n","             0.0000,      0.0000,     -0.0001,     -0.0001,     -0.0016,     -0.0053,     -0.0129,     -0.0253,     -0.0443,     -0.0704,\n","            -0.1023,     -0.1375,     -0.1718,     -0.1993,     -0.2127,     -0.2177,     -0.2163,     -0.2017,     -0.1668,     -0.1268,\n","            -0.0904,     -0.0595,     -0.0370,     -0.0212,     -0.0099,     -0.0027,     -0.0004,      0.0000,      0.0000,     -0.0000,\n","            -0.0001,     -0.0009,     -0.0043,     -0.0118,     -0.0265,     -0.0492,     -0.0816,     -0.1232,     -0.1691,     -0.2128,\n","            -0.2501,     -0.2805,     -0.2973,     -0.3079,     -0.3098,     -0.2918,     -0.2473,     -0.1897,     -0.1354,     -0.0904,\n","            -0.0576,     -0.0349,     -0.0181,     -0.0063,     -0.0013,     -0.0001,      0.0000,     -0.0001,     -0.0004,     -0.0023,\n","            -0.0090,     -0.0214,     -0.0441,     -0.0767,     -0.1216,     -0.1757,     -0.2284,     -0.2732,     -0.3072,     -0.3288,\n","            -0.3387,     -0.3488,     -0.3564,     -0.3462,     -0.3033,     -0.2392,     -0.1697,     -0.1102,     -0.0671,     -0.0414,\n","            -0.0226,     -0.0085,     -0.0017,     -0.0001,      0.0000,     -0.0003,     -0.0015,     -0.0053,     -0.0143,     -0.0310,\n","            -0.0589,     -0.0995,     -0.1536,     -0.2146,     -0.2690,     -0.3043,     -0.3194,     -0.3190,     -0.3156,     -0.3259,\n","            -0.3446,     -0.3547,     -0.3289,     -0.2642,     -0.1822,     -0.1119,     -0.0625,     -0.0353,     -0.0204,     -0.0088,\n","            -0.0017,     -0.0001,     -0.0001,     -0.0004,     -0.0022,     -0.0069,     -0.0166,     -0.0345,     -0.0656,     -0.1110,\n","            -0.1706,     -0.2307,     -0.2765,     -0.2941,     -0.2825,     -0.2599,     -0.2564,     -0.2788,     -0.3173,     -0.3509,\n","            -0.3365,     -0.2681,     -0.1751,     -0.0963,     -0.0454,     -0.0211,     -0.0135,     -0.0064,     -0.0011,     -0.0001,\n","            -0.0001,     -0.0004,     -0.0021,     -0.0064,     -0.0153,     -0.0323,     -0.0640,     -0.1101,     -0.1696,     -0.2250,\n","            -0.2611,     -0.2580,     -0.2288,     -0.2074,     -0.2215,     -0.2613,     -0.3156,     -0.3600,     -0.3438,     -0.2627,\n","            -0.1552,     -0.0721,     -0.0235,     -0.0056,     -0.0053,     -0.0039,     -0.0008,     -0.0001,     -0.0000,     -0.0003,\n","            -0.0017,     -0.0048,     -0.0123,     -0.0282,     -0.0582,     -0.1027,     -0.1605,     -0.2162,     -0.2454,     -0.2367,\n","            -0.2112,     -0.2085,     -0.2413,     -0.2909,     -0.3492,     -0.3888,     -0.3574,     -0.2536,     -0.1328,     -0.0473,\n","            -0.0064,      0.0046,      0.0002,     -0.0018,     -0.0004,     -0.0001,     -0.0000,     -0.0001,     -0.0010,     -0.0032,\n","            -0.0092,     -0.0236,     -0.0517,     -0.0967,     -0.1553,     -0.2129,     -0.2462,     -0.2454,     -0.2370,     -0.2607,\n","            -0.3101,     -0.3634,     -0.4154,     -0.4322,     -0.3700,     -0.2428,     -0.1125,     -0.0307,      0.0016,      0.0076,\n","             0.0014,     -0.0012,     -0.0003,     -0.0001,     -0.0000,     -0.0000,     -0.0005,     -0.0021,     -0.0070,     -0.0197,\n","            -0.0464,     -0.0914,     -0.1525,     -0.2139,     -0.2539,     -0.2683,     -0.2859,     -0.3398,     -0.3998,     -0.4516,\n","            -0.4790,     -0.4639,     -0.3726,     -0.2299,     -0.0996,     -0.0257,      0.0009,      0.0033,     -0.0009,     -0.0015,\n","            -0.0003,     -0.0001,      0.0000,     -0.0000,     -0.0002,     -0.0015,     -0.0055,     -0.0162,     -0.0407,     -0.0845,\n","            -0.1478,     -0.2146,     -0.2644,     -0.2956,     -0.3386,     -0.4138,     -0.4723,     -0.5104,     -0.5026,     -0.4622,\n","            -0.3572,     -0.2145,     -0.0964,     -0.0295,     -0.0066,     -0.0043,     -0.0040,     -0.0020,     -0.0002,     -0.0000,\n","             0.0000,      0.0000,     -0.0001,     -0.0015,     -0.0042,     -0.0131,     -0.0356,     -0.0753,     -0.1374,     -0.2113,\n","            -0.2722,     -0.3201,     -0.3785,     -0.4484,     -0.4924,     -0.5050,     -0.4775,     -0.4267,     -0.3231,     -0.1985,\n","            -0.0989,     -0.0400,     -0.0185,     -0.0118,     -0.0068,     -0.0029,     -0.0003,     -0.0000,      0.0000,      0.0000,\n","            -0.0002,     -0.0021,     -0.0044,     -0.0125,     -0.0322,     -0.0656,     -0.1235,     -0.1982,     -0.2643,     -0.3174,\n","            -0.3710,     -0.4215,     -0.4538,     -0.4543,     -0.4269,     -0.3727,     -0.2822,     -0.1835,     -0.1027,     -0.0530,\n","            -0.0310,     -0.0187,     -0.0098,     -0.0040,     -0.0005,     -0.0001,      0.0000,     -0.0000,     -0.0004,     -0.0033,\n","            -0.0064,     -0.0146,     -0.0323,     -0.0585,     -0.1063,     -0.1751,     -0.2399,     -0.2902,     -0.3285,     -0.3662,\n","            -0.3938,     -0.3954,     -0.3731,     -0.3227,     -0.2495,     -0.1732,     -0.1092,     -0.0666,     -0.0411,     -0.0236,\n","            -0.0118,     -0.0045,     -0.0006,     -0.0001,     -0.0000,     -0.0000,     -0.0007,     -0.0053,     -0.0114,     -0.0217,\n","            -0.0388,     -0.0602,     -0.0981,     -0.1585,     -0.2211,     -0.2658,     -0.2941,     -0.3271,     -0.3523,     -0.3593,\n","            -0.3393,     -0.2930,     -0.2331,     -0.1705,     -0.1163,     -0.0765,     -0.0482,     -0.0270,     -0.0128,     -0.0045,\n","            -0.0005,     -0.0000,      0.0000,     -0.0000,     -0.0011,     -0.0074,     -0.0171,     -0.0319,     -0.0535,     -0.0742,\n","            -0.1077,     -0.1596,     -0.2166,     -0.2570,     -0.2871,     -0.3151,     -0.3391,     -0.3457,     -0.3238,     -0.2784,\n","            -0.2229,     -0.1667,     -0.1171,     -0.0778,     -0.0480,     -0.0257,     -0.0115,     -0.0037,     -0.0005,      0.0000,\n","             0.0000,     -0.0001,     -0.0013,     -0.0085,     -0.0212,     -0.0392,     -0.0659,     -0.0942,     -0.1286,     -0.1727,\n","            -0.2198,     -0.2627,     -0.2975,     -0.3267,     -0.3451,     -0.3422,     -0.3113,     -0.2616,     -0.2066,     -0.1531,\n","            -0.1046,     -0.0675,     -0.0397,     -0.0207,     -0.0090,     -0.0027,     -0.0003,     -0.0000,     -0.0000,     -0.0001,\n","            -0.0011,     -0.0071,     -0.0190,     -0.0391,     -0.0692,     -0.1065,     -0.1467,     -0.1881,     -0.2300,     -0.2738,\n","            -0.3123,     -0.3401,     -0.3462,     -0.3280,     -0.2874,     -0.2320,     -0.1754,     -0.1247,     -0.0825,     -0.0504,\n","            -0.0284,     -0.0144,     -0.0059,     -0.0014,     -0.0001,      0.0000,     -0.0000,     -0.0000,     -0.0007,     -0.0042,\n","            -0.0126,     -0.0290,     -0.0565,     -0.0941,     -0.1398,     -0.1865,     -0.2323,     -0.2735,     -0.3074,     -0.3215,\n","            -0.3149,     -0.2872,     -0.2405,     -0.1858,     -0.1329,     -0.0886,     -0.0554,     -0.0314,     -0.0166,     -0.0078,\n","            -0.0030,     -0.0007,     -0.0000,      0.0000,      0.0000,      0.0000,     -0.0003,     -0.0017,     -0.0060,     -0.0157,\n","            -0.0347,     -0.0648,     -0.1047,     -0.1511,     -0.1985,     -0.2391,     -0.2653,     -0.2708,     -0.2541,     -0.2207,\n","            -0.1766,     -0.1293,     -0.0858,     -0.0531,     -0.0306,     -0.0159,     -0.0079,     -0.0036,     -0.0011,     -0.0002,\n","            -0.0001,      0.0000,      0.0000,      0.0000,     -0.0001,     -0.0005,     -0.0018,     -0.0057,     -0.0146,     -0.0308,\n","            -0.0553,     -0.0872,     -0.1205,     -0.1480,     -0.1645,     -0.1649,     -0.1502,     -0.1271,     -0.0993,     -0.0711,\n","            -0.0455,     -0.0276,     -0.0152,     -0.0076,     -0.0035,     -0.0015,     -0.0003,     -0.0001,     -0.0000,      0.0000,\n","             0.0000,      0.0000,     -0.0001,     -0.0001,     -0.0005,     -0.0018,     -0.0050,     -0.0116,     -0.0225,     -0.0361,\n","            -0.0511,     -0.0628,     -0.0691,     -0.0686,     -0.0620,     -0.0531,     -0.0424,     -0.0313,     -0.0207,     -0.0124,\n","            -0.0067,     -0.0033,     -0.0015,     -0.0006,     -0.0001,     -0.0000,     -0.0000,      0.0000,      0.0000,      0.0000,\n","             0.0000,      0.0000,     -0.0001,     -0.0005,     -0.0020,     -0.0045,     -0.0085,     -0.0127,     -0.0181,     -0.0234,\n","            -0.0258,     -0.0252,     -0.0223,     -0.0183,     -0.0148,     -0.0108,     -0.0070,     -0.0043,     -0.0022,     -0.0009,\n","            -0.0004,     -0.0001,     -0.0000,     -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n","             0.0000,     -0.0000,     -0.0002,     -0.0003,     -0.0005,     -0.0007,     -0.0012,     -0.0016,     -0.0020,     -0.0022,\n","            -0.0027,     -0.0024,     -0.0019,     -0.0013,     -0.0009,     -0.0005,     -0.0002,     -0.0001,     -0.0001,     -0.0000,\n","             0.0000,      0.0000,      0.0000,      0.0000])"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["coeffs.grad"]},{"cell_type":"markdown","metadata":{},"source":["Note that each time we call `backward`, the gradients are actually *added* to whatever is in the `.grad` attribute. Let's try running the above steps again:"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:19.603236Z","iopub.status.busy":"2022-05-30T22:34:19.60283Z","iopub.status.idle":"2022-05-30T22:34:19.610594Z","shell.execute_reply":"2022-05-30T22:34:19.609647Z","shell.execute_reply.started":"2022-05-30T22:34:19.603206Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n","             0.0000,      0.0000,     -0.0001,     -0.0001,     -0.0000,     -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n","             0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n","             0.0000,      0.0000,     -0.0000,     -0.0001,     -0.0001,     -0.0002,     -0.0004,     -0.0005,     -0.0010,     -0.0014,\n","            -0.0015,     -0.0015,     -0.0015,     -0.0012,     -0.0010,     -0.0010,     -0.0008,     -0.0005,     -0.0004,     -0.0001,\n","            -0.0001,     -0.0001,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,     -0.0000,      0.0000,\n","            -0.0001,     -0.0001,     -0.0004,     -0.0009,     -0.0021,     -0.0038,     -0.0062,     -0.0097,     -0.0142,     -0.0193,\n","            -0.0243,     -0.0261,     -0.0226,     -0.0183,     -0.0150,     -0.0130,     -0.0089,     -0.0047,     -0.0023,     -0.0008,\n","            -0.0003,     -0.0001,      0.0000,      0.0000,      0.0000,      0.0000,     -0.0001,     -0.0000,     -0.0002,     -0.0006,\n","            -0.0016,     -0.0034,     -0.0076,     -0.0145,     -0.0239,     -0.0358,     -0.0491,     -0.0644,     -0.0775,     -0.0827,\n","            -0.0774,     -0.0701,     -0.0601,     -0.0460,     -0.0291,     -0.0164,     -0.0079,     -0.0030,     -0.0012,     -0.0002,\n","            -0.0000,      0.0000,      0.0000,     -0.0000,     -0.0001,     -0.0001,     -0.0007,     -0.0026,     -0.0070,     -0.0167,\n","            -0.0335,     -0.0565,     -0.0881,     -0.1247,     -0.1637,     -0.1987,     -0.2209,     -0.2266,     -0.2179,     -0.1939,\n","            -0.1581,     -0.1214,     -0.0832,     -0.0519,     -0.0290,     -0.0143,     -0.0059,     -0.0013,     -0.0002,      0.0000,\n","             0.0000,      0.0000,     -0.0001,     -0.0003,     -0.0031,     -0.0105,     -0.0258,     -0.0507,     -0.0886,     -0.1407,\n","            -0.2045,     -0.2749,     -0.3437,     -0.3986,     -0.4254,     -0.4355,     -0.4327,     -0.4034,     -0.3336,     -0.2536,\n","            -0.1809,     -0.1191,     -0.0740,     -0.0423,     -0.0198,     -0.0053,     -0.0008,      0.0000,      0.0000,     -0.0000,\n","            -0.0003,     -0.0017,     -0.0086,     -0.0236,     -0.0529,     -0.0985,     -0.1632,     -0.2463,     -0.3382,     -0.4255,\n","            -0.5002,     -0.5611,     -0.5947,     -0.6158,     -0.6197,     -0.5835,     -0.4946,     -0.3793,     -0.2709,     -0.1807,\n","            -0.1152,     -0.0697,     -0.0363,     -0.0127,     -0.0026,     -0.0002,      0.0000,     -0.0003,     -0.0008,     -0.0047,\n","            -0.0179,     -0.0429,     -0.0882,     -0.1534,     -0.2433,     -0.3514,     -0.4567,     -0.5465,     -0.6143,     -0.6575,\n","            -0.6773,     -0.6976,     -0.7129,     -0.6924,     -0.6066,     -0.4785,     -0.3394,     -0.2204,     -0.1342,     -0.0827,\n","            -0.0452,     -0.0171,     -0.0035,     -0.0001,      0.0000,     -0.0005,     -0.0030,     -0.0106,     -0.0286,     -0.0619,\n","            -0.1179,     -0.1990,     -0.3072,     -0.4291,     -0.5381,     -0.6086,     -0.6388,     -0.6379,     -0.6312,     -0.6518,\n","            -0.6893,     -0.7094,     -0.6578,     -0.5284,     -0.3644,     -0.2238,     -0.1251,     -0.0706,     -0.0408,     -0.0177,\n","            -0.0033,     -0.0002,     -0.0001,     -0.0008,     -0.0043,     -0.0137,     -0.0332,     -0.0691,     -0.1312,     -0.2219,\n","            -0.3413,     -0.4615,     -0.5529,     -0.5883,     -0.5650,     -0.5199,     -0.5129,     -0.5576,     -0.6346,     -0.7019,\n","            -0.6730,     -0.5362,     -0.3503,     -0.1927,     -0.0908,     -0.0423,     -0.0270,     -0.0129,     -0.0022,     -0.0003,\n","            -0.0001,     -0.0008,     -0.0042,     -0.0129,     -0.0307,     -0.0646,     -0.1280,     -0.2203,     -0.3392,     -0.4500,\n","            -0.5221,     -0.5160,     -0.4575,     -0.4148,     -0.4430,     -0.5225,     -0.6312,     -0.7200,     -0.6876,     -0.5255,\n","            -0.3104,     -0.1442,     -0.0471,     -0.0112,     -0.0107,     -0.0077,     -0.0016,     -0.0002,     -0.0001,     -0.0005,\n","            -0.0034,     -0.0096,     -0.0246,     -0.0563,     -0.1164,     -0.2055,     -0.3210,     -0.4325,     -0.4908,     -0.4733,\n","            -0.4224,     -0.4170,     -0.4827,     -0.5819,     -0.6985,     -0.7777,     -0.7149,     -0.5073,     -0.2656,     -0.0946,\n","            -0.0128,      0.0093,      0.0004,     -0.0037,     -0.0008,     -0.0002,     -0.0000,     -0.0003,     -0.0021,     -0.0064,\n","            -0.0185,     -0.0472,     -0.1033,     -0.1933,     -0.3105,     -0.4258,     -0.4924,     -0.4907,     -0.4740,     -0.5213,\n","            -0.6202,     -0.7269,     -0.8308,     -0.8644,     -0.7400,     -0.4857,     -0.2249,     -0.0614,      0.0031,      0.0151,\n","             0.0027,     -0.0024,     -0.0005,     -0.0002,     -0.0000,     -0.0001,     -0.0009,     -0.0042,     -0.0140,     -0.0394,\n","            -0.0928,     -0.1828,     -0.3050,     -0.4277,     -0.5077,     -0.5367,     -0.5718,     -0.6796,     -0.7997,     -0.9032,\n","            -0.9580,     -0.9279,     -0.7453,     -0.4598,     -0.1993,     -0.0514,      0.0019,      0.0066,     -0.0017,     -0.0030,\n","            -0.0006,     -0.0001,      0.0000,     -0.0001,     -0.0004,     -0.0030,     -0.0109,     -0.0324,     -0.0814,     -0.1691,\n","            -0.2956,     -0.4291,     -0.5287,     -0.5911,     -0.6771,     -0.8275,     -0.9445,     -1.0208,     -1.0053,     -0.9244,\n","            -0.7144,     -0.4290,     -0.1929,     -0.0590,     -0.0132,     -0.0086,     -0.0081,     -0.0040,     -0.0004,     -0.0001,\n","             0.0000,      0.0000,     -0.0003,     -0.0031,     -0.0083,     -0.0261,     -0.0713,     -0.1507,     -0.2749,     -0.4226,\n","            -0.5445,     -0.6402,     -0.7571,     -0.8967,     -0.9847,     -1.0101,     -0.9550,     -0.8533,     -0.6461,     -0.3969,\n","            -0.1978,     -0.0801,     -0.0371,     -0.0236,     -0.0137,     -0.0058,     -0.0006,     -0.0001,      0.0000,      0.0000,\n","            -0.0005,     -0.0041,     -0.0089,     -0.0249,     -0.0644,     -0.1311,     -0.2470,     -0.3965,     -0.5286,     -0.6348,\n","            -0.7421,     -0.8430,     -0.9077,     -0.9087,     -0.8539,     -0.7454,     -0.5644,     -0.3669,     -0.2053,     -0.1060,\n","            -0.0619,     -0.0373,     -0.0196,     -0.0079,     -0.0011,     -0.0001,      0.0000,     -0.0000,     -0.0008,     -0.0065,\n","            -0.0128,     -0.0291,     -0.0646,     -0.1171,     -0.2127,     -0.3503,     -0.4797,     -0.5804,     -0.6570,     -0.7324,\n","            -0.7876,     -0.7907,     -0.7462,     -0.6453,     -0.4991,     -0.3465,     -0.2184,     -0.1331,     -0.0821,     -0.0472,\n","            -0.0236,     -0.0091,     -0.0012,     -0.0003,     -0.0001,     -0.0001,     -0.0013,     -0.0107,     -0.0229,     -0.0433,\n","            -0.0776,     -0.1203,     -0.1962,     -0.3170,     -0.4422,     -0.5316,     -0.5881,     -0.6542,     -0.7046,     -0.7185,\n","            -0.6786,     -0.5860,     -0.4662,     -0.3411,     -0.2325,     -0.1530,     -0.0964,     -0.0540,     -0.0255,     -0.0090,\n","            -0.0011,     -0.0001,      0.0000,     -0.0001,     -0.0022,     -0.0148,     -0.0342,     -0.0638,     -0.1070,     -0.1484,\n","            -0.2153,     -0.3191,     -0.4332,     -0.5140,     -0.5742,     -0.6302,     -0.6781,     -0.6914,     -0.6476,     -0.5567,\n","            -0.4459,     -0.3334,     -0.2342,     -0.1556,     -0.0960,     -0.0515,     -0.0230,     -0.0073,     -0.0010,      0.0000,\n","             0.0000,     -0.0001,     -0.0025,     -0.0170,     -0.0423,     -0.0785,     -0.1317,     -0.1884,     -0.2571,     -0.3455,\n","            -0.4396,     -0.5255,     -0.5950,     -0.6534,     -0.6902,     -0.6845,     -0.6226,     -0.5233,     -0.4133,     -0.3062,\n","            -0.2092,     -0.1349,     -0.0794,     -0.0414,     -0.0180,     -0.0053,     -0.0007,     -0.0000,     -0.0000,     -0.0001,\n","            -0.0021,     -0.0142,     -0.0381,     -0.0782,     -0.1385,     -0.2130,     -0.2933,     -0.3761,     -0.4600,     -0.5476,\n","            -0.6246,     -0.6803,     -0.6924,     -0.6560,     -0.5749,     -0.4640,     -0.3507,     -0.2494,     -0.1651,     -0.1007,\n","            -0.0567,     -0.0288,     -0.0119,     -0.0029,     -0.0003,      0.0000,     -0.0000,     -0.0001,     -0.0013,     -0.0084,\n","            -0.0252,     -0.0581,     -0.1129,     -0.1881,     -0.2796,     -0.3731,     -0.4647,     -0.5470,     -0.6148,     -0.6431,\n","            -0.6299,     -0.5744,     -0.4810,     -0.3716,     -0.2658,     -0.1772,     -0.1108,     -0.0627,     -0.0332,     -0.0157,\n","            -0.0059,     -0.0014,     -0.0001,      0.0000,      0.0000,      0.0000,     -0.0006,     -0.0034,     -0.0120,     -0.0314,\n","            -0.0695,     -0.1296,     -0.2094,     -0.3022,     -0.3969,     -0.4781,     -0.5306,     -0.5416,     -0.5082,     -0.4414,\n","            -0.3532,     -0.2586,     -0.1715,     -0.1063,     -0.0611,     -0.0317,     -0.0158,     -0.0071,     -0.0023,     -0.0005,\n","            -0.0001,      0.0000,      0.0000,      0.0000,     -0.0001,     -0.0010,     -0.0037,     -0.0115,     -0.0291,     -0.0616,\n","            -0.1106,     -0.1743,     -0.2411,     -0.2960,     -0.3290,     -0.3297,     -0.3003,     -0.2543,     -0.1986,     -0.1422,\n","            -0.0911,     -0.0552,     -0.0305,     -0.0153,     -0.0071,     -0.0029,     -0.0007,     -0.0002,     -0.0001,      0.0000,\n","             0.0000,      0.0000,     -0.0001,     -0.0001,     -0.0010,     -0.0035,     -0.0100,     -0.0231,     -0.0451,     -0.0723,\n","            -0.1022,     -0.1255,     -0.1381,     -0.1372,     -0.1241,     -0.1061,     -0.0848,     -0.0627,     -0.0414,     -0.0248,\n","            -0.0134,     -0.0066,     -0.0030,     -0.0011,     -0.0002,     -0.0001,     -0.0000,      0.0000,      0.0000,      0.0000,\n","             0.0000,      0.0000,     -0.0003,     -0.0011,     -0.0039,     -0.0090,     -0.0170,     -0.0254,     -0.0363,     -0.0467,\n","            -0.0516,     -0.0504,     -0.0445,     -0.0365,     -0.0296,     -0.0216,     -0.0141,     -0.0085,     -0.0044,     -0.0019,\n","            -0.0007,     -0.0002,     -0.0001,     -0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n","             0.0000,     -0.0001,     -0.0003,     -0.0007,     -0.0009,     -0.0014,     -0.0024,     -0.0032,     -0.0040,     -0.0044,\n","            -0.0053,     -0.0047,     -0.0038,     -0.0027,     -0.0017,     -0.0009,     -0.0005,     -0.0002,     -0.0001,     -0.0001,\n","             0.0000,      0.0000,      0.0000,      0.0000])"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["loss = calc_loss(coeffs, t_indep, t_dep)\n","loss.backward()\n","coeffs.grad"]},{"cell_type":"markdown","metadata":{},"source":["As you see, our `.grad` values are have doubled. That's because it added the gradients a second time. For this reason, after we use the gradients to do a gradient descent step, we need to set them back to zero.\n","\n","We can now do one gradient descent step, and check that our loss decreases:"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:34:19.665739Z","iopub.status.busy":"2022-05-30T22:34:19.665238Z","iopub.status.idle":"2022-05-30T22:34:19.674839Z","shell.execute_reply":"2022-05-30T22:34:19.673727Z","shell.execute_reply.started":"2022-05-30T22:34:19.665705Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(4.4487)\n"]}],"source":["loss = calc_loss(coeffs, t_indep, t_dep)\n","loss.backward()\n","with torch.no_grad():\n","    coeffs.sub_(coeffs.grad * 0.1)\n","    coeffs.grad.zero_()\n","    print(calc_loss(coeffs, t_indep, t_dep))"]},{"cell_type":"markdown","metadata":{},"source":["Note that `a.sub_(b)` subtracts `b` from `a` in-place. In PyTorch, any method that ends in `_` changes its object in-place. Similarly, `a.zero_()` sets all elements of a tensor to zero."]},{"cell_type":"markdown","metadata":{},"source":["## Training the linear model"]},{"cell_type":"markdown","metadata":{},"source":["Before we begin training our model, we'll need to ensure that we hold out a validation set for calculating our metrics (for details on this, see \"[Getting started with NLP for absolute beginners](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners#Test-and-validation-sets)\".\n","\n","There's lots of different ways we can do this. In the next notebook we'll be comparing our approach here to what the fastai library does, so we'll want to ensure we split the data in the same way. So let's use `RandomSplitter` to get indices that will split our data into training and validation sets:"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:35:26.57635Z","iopub.status.busy":"2022-05-30T22:35:26.57605Z","iopub.status.idle":"2022-05-30T22:35:27.821258Z","shell.execute_reply":"2022-05-30T22:35:27.820329Z","shell.execute_reply.started":"2022-05-30T22:35:26.57632Z"},"trusted":true},"outputs":[],"source":["from fastai.data.transforms import RandomSplitter\n","trn_split,val_split=RandomSplitter(seed=42)(df)"]},{"cell_type":"markdown","metadata":{},"source":["Now we can apply those indicies to our independent and dependent variables:"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:35:27.822887Z","iopub.status.busy":"2022-05-30T22:35:27.822671Z","iopub.status.idle":"2022-05-30T22:35:27.836937Z","shell.execute_reply":"2022-05-30T22:35:27.836081Z","shell.execute_reply.started":"2022-05-30T22:35:27.822859Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(33600, 8400)"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["trn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\n","trn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\n","len(trn_indep),len(val_indep)"]},{"cell_type":"markdown","metadata":{},"source":["We'll create functions for the three things we did manually above: updating `coeffs`, doing one full gradient descent step, and initilising `coeffs` to random numbers:"]},{"cell_type":"code","execution_count":85,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:35:35.029177Z","iopub.status.busy":"2022-05-30T22:35:35.028649Z","iopub.status.idle":"2022-05-30T22:35:35.034346Z","shell.execute_reply":"2022-05-30T22:35:35.033261Z","shell.execute_reply.started":"2022-05-30T22:35:35.029143Z"},"trusted":true},"outputs":[],"source":["def update_coeffs(coeffs, lr):\n","    coeffs.sub_(coeffs.grad * lr)\n","    coeffs.grad.zero_()"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:35:35.633995Z","iopub.status.busy":"2022-05-30T22:35:35.633703Z","iopub.status.idle":"2022-05-30T22:35:35.639103Z","shell.execute_reply":"2022-05-30T22:35:35.63814Z","shell.execute_reply.started":"2022-05-30T22:35:35.633964Z"},"trusted":true},"outputs":[],"source":["def one_epoch(coeffs, lr):\n","    loss = calc_loss(coeffs, trn_indep, trn_dep)\n","    loss.backward()\n","    with torch.no_grad(): update_coeffs(coeffs, lr)\n","    print(f\"{loss:.3f}\", end=\"; \")"]},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:35:36.293837Z","iopub.status.busy":"2022-05-30T22:35:36.293565Z","iopub.status.idle":"2022-05-30T22:35:36.297457Z","shell.execute_reply":"2022-05-30T22:35:36.296816Z","shell.execute_reply.started":"2022-05-30T22:35:36.293808Z"},"trusted":true},"outputs":[],"source":["def init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()"]},{"cell_type":"markdown","metadata":{},"source":["We can now use these functions to train our model:"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:35:38.772314Z","iopub.status.busy":"2022-05-30T22:35:38.771827Z","iopub.status.idle":"2022-05-30T22:35:38.777313Z","shell.execute_reply":"2022-05-30T22:35:38.776439Z","shell.execute_reply.started":"2022-05-30T22:35:38.772247Z"},"trusted":true},"outputs":[],"source":["def train_model(epochs=30, lr=0.01):\n","    torch.manual_seed(442)\n","    coeffs = init_coeffs()\n","    for i in range(epochs): one_epoch(coeffs, lr=lr)\n","    return coeffs"]},{"cell_type":"markdown","metadata":{},"source":["Let's try it. Our loss will print at the end of every step, so we hope we'll see it going down:"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:36:14.16788Z","iopub.status.busy":"2022-05-30T22:36:14.16706Z","iopub.status.idle":"2022-05-30T22:36:14.181652Z","shell.execute_reply":"2022-05-30T22:36:14.180496Z","shell.execute_reply.started":"2022-05-30T22:36:14.167812Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["4.311; 2.890; 2.667; 2.586; 2.515; 2.451; 2.394; 2.342; 2.295; 2.254; 2.216; 2.183; 2.152; 2.125; 2.100; 2.078; 2.059; 2.041; "]}],"source":["coeffs = train_model(18, lr=0.1)"]},{"cell_type":"markdown","metadata":{},"source":["It does!\n","\n","Let's take a look at the coefficients for each column:"]},{"cell_type":"code","execution_count":91,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:36:21.980389Z","iopub.status.busy":"2022-05-30T22:36:21.97957Z","iopub.status.idle":"2022-05-30T22:36:21.990088Z","shell.execute_reply":"2022-05-30T22:36:21.989021Z","shell.execute_reply.started":"2022-05-30T22:36:21.98035Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'pixel0': tensor(-0.4629),\n"," 'pixel1': tensor(0.1386),\n"," 'pixel2': tensor(0.2409),\n"," 'pixel3': tensor(-0.2262),\n"," 'pixel4': tensor(-0.2632),\n"," 'pixel5': tensor(-0.3147),\n"," 'pixel6': tensor(0.4876),\n"," 'pixel7': tensor(0.3136),\n"," 'pixel8': tensor(0.2799),\n"," 'pixel9': tensor(-0.4392),\n"," 'pixel10': tensor(0.2103),\n"," 'pixel11': tensor(0.3625),\n"," 'pixel12': tensor(0.1722),\n"," 'pixel13': tensor(0.2325),\n"," 'pixel14': tensor(-0.3575),\n"," 'pixel15': tensor(-0.0010),\n"," 'pixel16': tensor(-0.1833),\n"," 'pixel17': tensor(-0.2411),\n"," 'pixel18': tensor(0.0489),\n"," 'pixel19': tensor(0.0866),\n"," 'pixel20': tensor(-0.0534),\n"," 'pixel21': tensor(0.3132),\n"," 'pixel22': tensor(-0.1487),\n"," 'pixel23': tensor(-0.2551),\n"," 'pixel24': tensor(0.3328),\n"," 'pixel25': tensor(0.1292),\n"," 'pixel26': tensor(0.2361),\n"," 'pixel27': tensor(-0.2622),\n"," 'pixel28': tensor(0.4051),\n"," 'pixel29': tensor(-0.2674),\n"," 'pixel30': tensor(-0.2312),\n"," 'pixel31': tensor(0.1147),\n"," 'pixel32': tensor(0.4073),\n"," 'pixel33': tensor(0.2835),\n"," 'pixel34': tensor(0.0836),\n"," 'pixel35': tensor(-0.0502),\n"," 'pixel36': tensor(0.4357),\n"," 'pixel37': tensor(-0.3404),\n"," 'pixel38': tensor(-0.4544),\n"," 'pixel39': tensor(0.1459),\n"," 'pixel40': tensor(-0.4636),\n"," 'pixel41': tensor(-0.3028),\n"," 'pixel42': tensor(-0.1286),\n"," 'pixel43': tensor(-0.0842),\n"," 'pixel44': tensor(0.4684),\n"," 'pixel45': tensor(0.3973),\n"," 'pixel46': tensor(-0.4849),\n"," 'pixel47': tensor(0.2100),\n"," 'pixel48': tensor(-0.3016),\n"," 'pixel49': tensor(0.4186),\n"," 'pixel50': tensor(0.1090),\n"," 'pixel51': tensor(0.0791),\n"," 'pixel52': tensor(-0.1708),\n"," 'pixel53': tensor(-0.1882),\n"," 'pixel54': tensor(0.4861),\n"," 'pixel55': tensor(-0.2534),\n"," 'pixel56': tensor(-0.2502),\n"," 'pixel57': tensor(-0.0770),\n"," 'pixel58': tensor(0.3336),\n"," 'pixel59': tensor(-0.4976),\n"," 'pixel60': tensor(-0.2560),\n"," 'pixel61': tensor(0.0892),\n"," 'pixel62': tensor(-0.4669),\n"," 'pixel63': tensor(0.3730),\n"," 'pixel64': tensor(-0.1085),\n"," 'pixel65': tensor(0.3465),\n"," 'pixel66': tensor(-0.3408),\n"," 'pixel67': tensor(0.1169),\n"," 'pixel68': tensor(-0.2795),\n"," 'pixel69': tensor(0.0322),\n"," 'pixel70': tensor(-0.1566),\n"," 'pixel71': tensor(-0.0808),\n"," 'pixel72': tensor(-0.1347),\n"," 'pixel73': tensor(0.3624),\n"," 'pixel74': tensor(0.3703),\n"," 'pixel75': tensor(0.4775),\n"," 'pixel76': tensor(-0.0299),\n"," 'pixel77': tensor(-0.3049),\n"," 'pixel78': tensor(0.1466),\n"," 'pixel79': tensor(0.3734),\n"," 'pixel80': tensor(0.1067),\n"," 'pixel81': tensor(-0.1043),\n"," 'pixel82': tensor(-0.4297),\n"," 'pixel83': tensor(-0.0069),\n"," 'pixel84': tensor(-0.2685),\n"," 'pixel85': tensor(0.2132),\n"," 'pixel86': tensor(-0.0946),\n"," 'pixel87': tensor(-0.4802),\n"," 'pixel88': tensor(0.1940),\n"," 'pixel89': tensor(0.0919),\n"," 'pixel90': tensor(0.2913),\n"," 'pixel91': tensor(-0.2693),\n"," 'pixel92': tensor(0.4456),\n"," 'pixel93': tensor(-0.2982),\n"," 'pixel94': tensor(0.1611),\n"," 'pixel95': tensor(0.3253),\n"," 'pixel96': tensor(-0.1431),\n"," 'pixel97': tensor(0.2970),\n"," 'pixel98': tensor(0.0721),\n"," 'pixel99': tensor(0.2699),\n"," 'pixel100': tensor(0.2281),\n"," 'pixel101': tensor(-0.2777),\n"," 'pixel102': tensor(-0.3186),\n"," 'pixel103': tensor(-0.3555),\n"," 'pixel104': tensor(0.1155),\n"," 'pixel105': tensor(-0.2848),\n"," 'pixel106': tensor(-0.3828),\n"," 'pixel107': tensor(-0.3095),\n"," 'pixel108': tensor(-0.1853),\n"," 'pixel109': tensor(0.3500),\n"," 'pixel110': tensor(-0.4067),\n"," 'pixel111': tensor(-0.3355),\n"," 'pixel112': tensor(-0.4195),\n"," 'pixel113': tensor(0.4183),\n"," 'pixel114': tensor(0.0745),\n"," 'pixel115': tensor(-0.2618),\n"," 'pixel116': tensor(0.1314),\n"," 'pixel117': tensor(0.1040),\n"," 'pixel118': tensor(-0.1727),\n"," 'pixel119': tensor(0.2862),\n"," 'pixel120': tensor(0.1239),\n"," 'pixel121': tensor(-0.1236),\n"," 'pixel122': tensor(-0.3599),\n"," 'pixel123': tensor(-0.1848),\n"," 'pixel124': tensor(0.2935),\n"," 'pixel125': tensor(-0.3326),\n"," 'pixel126': tensor(-0.0025),\n"," 'pixel127': tensor(0.1159),\n"," 'pixel128': tensor(-0.1792),\n"," 'pixel129': tensor(0.4138),\n"," 'pixel130': tensor(-0.0104),\n"," 'pixel131': tensor(-0.2564),\n"," 'pixel132': tensor(-0.3682),\n"," 'pixel133': tensor(0.0827),\n"," 'pixel134': tensor(0.0161),\n"," 'pixel135': tensor(0.2890),\n"," 'pixel136': tensor(0.0115),\n"," 'pixel137': tensor(0.2866),\n"," 'pixel138': tensor(-0.2500),\n"," 'pixel139': tensor(-0.3194),\n"," 'pixel140': tensor(-0.0428),\n"," 'pixel141': tensor(0.2932),\n"," 'pixel142': tensor(-0.1903),\n"," 'pixel143': tensor(-0.4143),\n"," 'pixel144': tensor(-0.1951),\n"," 'pixel145': tensor(-0.1332),\n"," 'pixel146': tensor(-0.1367),\n"," 'pixel147': tensor(-0.0866),\n"," 'pixel148': tensor(-0.3279),\n"," 'pixel149': tensor(0.0949),\n"," 'pixel150': tensor(-0.2557),\n"," 'pixel151': tensor(0.1791),\n"," 'pixel152': tensor(-0.2245),\n"," 'pixel153': tensor(0.0098),\n"," 'pixel154': tensor(-0.2588),\n"," 'pixel155': tensor(-0.0608),\n"," 'pixel156': tensor(0.3213),\n"," 'pixel157': tensor(-0.3329),\n"," 'pixel158': tensor(-0.0967),\n"," 'pixel159': tensor(-0.0945),\n"," 'pixel160': tensor(-0.0244),\n"," 'pixel161': tensor(0.4307),\n"," 'pixel162': tensor(-0.3233),\n"," 'pixel163': tensor(0.4614),\n"," 'pixel164': tensor(0.4944),\n"," 'pixel165': tensor(0.0707),\n"," 'pixel166': tensor(0.4649),\n"," 'pixel167': tensor(0.3530),\n"," 'pixel168': tensor(0.0762),\n"," 'pixel169': tensor(0.1317),\n"," 'pixel170': tensor(0.1662),\n"," 'pixel171': tensor(0.0824),\n"," 'pixel172': tensor(-0.4619),\n"," 'pixel173': tensor(-0.3938),\n"," 'pixel174': tensor(-0.3625),\n"," 'pixel175': tensor(-0.0833),\n"," 'pixel176': tensor(-0.1951),\n"," 'pixel177': tensor(0.2541),\n"," 'pixel178': tensor(-0.4281),\n"," 'pixel179': tensor(-0.4539),\n"," 'pixel180': tensor(0.3392),\n"," 'pixel181': tensor(-0.4371),\n"," 'pixel182': tensor(0.1763),\n"," 'pixel183': tensor(0.3401),\n"," 'pixel184': tensor(-0.1311),\n"," 'pixel185': tensor(-0.2273),\n"," 'pixel186': tensor(-0.0134),\n"," 'pixel187': tensor(-0.4310),\n"," 'pixel188': tensor(0.3625),\n"," 'pixel189': tensor(-0.2051),\n"," 'pixel190': tensor(-0.0615),\n"," 'pixel191': tensor(-0.2821),\n"," 'pixel192': tensor(-0.2029),\n"," 'pixel193': tensor(0.4620),\n"," 'pixel194': tensor(-0.1486),\n"," 'pixel195': tensor(-0.3910),\n"," 'pixel196': tensor(0.3088),\n"," 'pixel197': tensor(-0.0718),\n"," 'pixel198': tensor(-0.4046),\n"," 'pixel199': tensor(0.3443),\n"," 'pixel200': tensor(0.1568),\n"," 'pixel201': tensor(0.0245),\n"," 'pixel202': tensor(-0.4675),\n"," 'pixel203': tensor(0.3024),\n"," 'pixel204': tensor(0.0964),\n"," 'pixel205': tensor(0.1097),\n"," 'pixel206': tensor(-0.1849),\n"," 'pixel207': tensor(0.0785),\n"," 'pixel208': tensor(-0.3751),\n"," 'pixel209': tensor(0.3517),\n"," 'pixel210': tensor(0.3458),\n"," 'pixel211': tensor(0.2152),\n"," 'pixel212': tensor(-0.1948),\n"," 'pixel213': tensor(-0.3179),\n"," 'pixel214': tensor(-0.2092),\n"," 'pixel215': tensor(0.4573),\n"," 'pixel216': tensor(-0.0701),\n"," 'pixel217': tensor(-0.4888),\n"," 'pixel218': tensor(0.3245),\n"," 'pixel219': tensor(-0.4015),\n"," 'pixel220': tensor(-0.3963),\n"," 'pixel221': tensor(0.2202),\n"," 'pixel222': tensor(-0.4296),\n"," 'pixel223': tensor(-0.4857),\n"," 'pixel224': tensor(0.0427),\n"," 'pixel225': tensor(-0.0671),\n"," 'pixel226': tensor(-0.1155),\n"," 'pixel227': tensor(0.2780),\n"," 'pixel228': tensor(-0.4332),\n"," 'pixel229': tensor(-0.2445),\n"," 'pixel230': tensor(0.2286),\n"," 'pixel231': tensor(0.3651),\n"," 'pixel232': tensor(-0.3480),\n"," 'pixel233': tensor(-0.2352),\n"," 'pixel234': tensor(-0.3106),\n"," 'pixel235': tensor(0.1845),\n"," 'pixel236': tensor(0.3653),\n"," 'pixel237': tensor(-0.3054),\n"," 'pixel238': tensor(0.4378),\n"," 'pixel239': tensor(0.0492),\n"," 'pixel240': tensor(0.4472),\n"," 'pixel241': tensor(0.2053),\n"," 'pixel242': tensor(0.0126),\n"," 'pixel243': tensor(-0.2451),\n"," 'pixel244': tensor(-0.4610),\n"," 'pixel245': tensor(-0.1456),\n"," 'pixel246': tensor(0.1433),\n"," 'pixel247': tensor(0.0404),\n"," 'pixel248': tensor(0.3937),\n"," 'pixel249': tensor(0.0523),\n"," 'pixel250': tensor(-0.1024),\n"," 'pixel251': tensor(-0.3455),\n"," 'pixel252': tensor(-0.2748),\n"," 'pixel253': tensor(0.1898),\n"," 'pixel254': tensor(-0.3435),\n"," 'pixel255': tensor(-0.2554),\n"," 'pixel256': tensor(0.4196),\n"," 'pixel257': tensor(0.4374),\n"," 'pixel258': tensor(0.3552),\n"," 'pixel259': tensor(-0.4169),\n"," 'pixel260': tensor(0.3298),\n"," 'pixel261': tensor(-0.2645),\n"," 'pixel262': tensor(0.5017),\n"," 'pixel263': tensor(-0.3920),\n"," 'pixel264': tensor(-0.0349),\n"," 'pixel265': tensor(0.1645),\n"," 'pixel266': tensor(0.1573),\n"," 'pixel267': tensor(0.1889),\n"," 'pixel268': tensor(0.1957),\n"," 'pixel269': tensor(-0.3310),\n"," 'pixel270': tensor(0.0972),\n"," 'pixel271': tensor(-0.2037),\n"," 'pixel272': tensor(0.4566),\n"," 'pixel273': tensor(0.3039),\n"," 'pixel274': tensor(0.2619),\n"," 'pixel275': tensor(0.4822),\n"," 'pixel276': tensor(-0.3737),\n"," 'pixel277': tensor(0.3804),\n"," 'pixel278': tensor(-0.1321),\n"," 'pixel279': tensor(0.0470),\n"," 'pixel280': tensor(0.0856),\n"," 'pixel281': tensor(0.2169),\n"," 'pixel282': tensor(-0.2436),\n"," 'pixel283': tensor(-0.4545),\n"," 'pixel284': tensor(0.3133),\n"," 'pixel285': tensor(0.4208),\n"," 'pixel286': tensor(0.0120),\n"," 'pixel287': tensor(-0.4291),\n"," 'pixel288': tensor(0.0597),\n"," 'pixel289': tensor(0.4487),\n"," 'pixel290': tensor(0.0418),\n"," 'pixel291': tensor(0.4814),\n"," 'pixel292': tensor(-0.2359),\n"," 'pixel293': tensor(0.3526),\n"," 'pixel294': tensor(-0.3834),\n"," 'pixel295': tensor(-0.3075),\n"," 'pixel296': tensor(-0.3014),\n"," 'pixel297': tensor(-0.1297),\n"," 'pixel298': tensor(0.3827),\n"," 'pixel299': tensor(0.4978),\n"," 'pixel300': tensor(-0.0922),\n"," 'pixel301': tensor(-0.2892),\n"," 'pixel302': tensor(0.4075),\n"," 'pixel303': tensor(0.1536),\n"," 'pixel304': tensor(-0.1757),\n"," 'pixel305': tensor(-0.1020),\n"," 'pixel306': tensor(0.0484),\n"," 'pixel307': tensor(-0.2267),\n"," 'pixel308': tensor(-0.3953),\n"," 'pixel309': tensor(-0.1489),\n"," 'pixel310': tensor(0.3904),\n"," 'pixel311': tensor(-0.4640),\n"," 'pixel312': tensor(0.2586),\n"," 'pixel313': tensor(-0.1398),\n"," 'pixel314': tensor(-0.2643),\n"," 'pixel315': tensor(-0.0319),\n"," 'pixel316': tensor(0.2050),\n"," 'pixel317': tensor(-0.0889),\n"," 'pixel318': tensor(-0.3124),\n"," 'pixel319': tensor(0.1778),\n"," 'pixel320': tensor(0.1033),\n"," 'pixel321': tensor(-0.4058),\n"," 'pixel322': tensor(-0.4056),\n"," 'pixel323': tensor(0.1458),\n"," 'pixel324': tensor(0.0923),\n"," 'pixel325': tensor(-0.1201),\n"," 'pixel326': tensor(-0.1642),\n"," 'pixel327': tensor(0.4017),\n"," 'pixel328': tensor(0.4406),\n"," 'pixel329': tensor(-0.4610),\n"," 'pixel330': tensor(-0.2916),\n"," 'pixel331': tensor(-0.1584),\n"," 'pixel332': tensor(0.1190),\n"," 'pixel333': tensor(0.4039),\n"," 'pixel334': tensor(0.4520),\n"," 'pixel335': tensor(-0.3047),\n"," 'pixel336': tensor(0.2062),\n"," 'pixel337': tensor(-0.3029),\n"," 'pixel338': tensor(-0.2600),\n"," 'pixel339': tensor(0.3967),\n"," 'pixel340': tensor(-0.3830),\n"," 'pixel341': tensor(-0.1215),\n"," 'pixel342': tensor(-0.0141),\n"," 'pixel343': tensor(0.4566),\n"," 'pixel344': tensor(0.2541),\n"," 'pixel345': tensor(0.1972),\n"," 'pixel346': tensor(-0.0369),\n"," 'pixel347': tensor(-0.0850),\n"," 'pixel348': tensor(-0.1200),\n"," 'pixel349': tensor(0.1326),\n"," 'pixel350': tensor(-0.4270),\n"," 'pixel351': tensor(0.2887),\n"," 'pixel352': tensor(0.3996),\n"," 'pixel353': tensor(0.5144),\n"," 'pixel354': tensor(-0.2513),\n"," 'pixel355': tensor(0.0511),\n"," 'pixel356': tensor(0.1546),\n"," 'pixel357': tensor(0.1012),\n"," 'pixel358': tensor(-0.3570),\n"," 'pixel359': tensor(0.3552),\n"," 'pixel360': tensor(-0.4624),\n"," 'pixel361': tensor(-0.3867),\n"," 'pixel362': tensor(-0.0916),\n"," 'pixel363': tensor(-0.2868),\n"," 'pixel364': tensor(0.2481),\n"," 'pixel365': tensor(-0.2724),\n"," 'pixel366': tensor(-0.1472),\n"," 'pixel367': tensor(-0.0949),\n"," 'pixel368': tensor(0.4782),\n"," 'pixel369': tensor(0.3767),\n"," 'pixel370': tensor(-0.2554),\n"," 'pixel371': tensor(-0.2299),\n"," 'pixel372': tensor(0.0780),\n"," 'pixel373': tensor(0.3059),\n"," 'pixel374': tensor(0.2461),\n"," 'pixel375': tensor(0.5026),\n"," 'pixel376': tensor(-0.0778),\n"," 'pixel377': tensor(0.0916),\n"," 'pixel378': tensor(-0.3143),\n"," 'pixel379': tensor(0.0674),\n"," 'pixel380': tensor(0.5582),\n"," 'pixel381': tensor(0.0566),\n"," 'pixel382': tensor(-0.3252),\n"," 'pixel383': tensor(-0.0087),\n"," 'pixel384': tensor(-0.1298),\n"," 'pixel385': tensor(-0.4943),\n"," 'pixel386': tensor(0.1348),\n"," 'pixel387': tensor(-0.0829),\n"," 'pixel388': tensor(0.4123),\n"," 'pixel389': tensor(0.0900),\n"," 'pixel390': tensor(-0.3155),\n"," 'pixel391': tensor(-0.4250),\n"," 'pixel392': tensor(-0.1012),\n"," 'pixel393': tensor(-0.0953),\n"," 'pixel394': tensor(-0.1273),\n"," 'pixel395': tensor(0.3102),\n"," 'pixel396': tensor(0.4937),\n"," 'pixel397': tensor(0.0270),\n"," 'pixel398': tensor(0.1746),\n"," 'pixel399': tensor(0.2412),\n"," 'pixel400': tensor(-0.4150),\n"," 'pixel401': tensor(-0.1542),\n"," 'pixel402': tensor(-0.0511),\n"," 'pixel403': tensor(0.4746),\n"," 'pixel404': tensor(0.4635),\n"," 'pixel405': tensor(-0.1431),\n"," 'pixel406': tensor(0.1642),\n"," 'pixel407': tensor(-0.2668),\n"," 'pixel408': tensor(0.3386),\n"," 'pixel409': tensor(0.3441),\n"," 'pixel410': tensor(0.1208),\n"," 'pixel411': tensor(0.1744),\n"," 'pixel412': tensor(0.0917),\n"," 'pixel413': tensor(0.0457),\n"," 'pixel414': tensor(0.3397),\n"," 'pixel415': tensor(-0.4664),\n"," 'pixel416': tensor(-0.3029),\n"," 'pixel417': tensor(0.1250),\n"," 'pixel418': tensor(0.1589),\n"," 'pixel419': tensor(0.3702),\n"," 'pixel420': tensor(0.0802),\n"," 'pixel421': tensor(0.1926),\n"," 'pixel422': tensor(-0.2709),\n"," 'pixel423': tensor(-0.4387),\n"," 'pixel424': tensor(0.0704),\n"," 'pixel425': tensor(-0.4830),\n"," 'pixel426': tensor(-0.2563),\n"," 'pixel427': tensor(-0.0841),\n"," 'pixel428': tensor(0.3256),\n"," 'pixel429': tensor(-0.2640),\n"," 'pixel430': tensor(0.0778),\n"," 'pixel431': tensor(0.1327),\n"," 'pixel432': tensor(-0.0752),\n"," 'pixel433': tensor(0.5550),\n"," 'pixel434': tensor(-0.1462),\n"," 'pixel435': tensor(-0.3004),\n"," 'pixel436': tensor(0.4718),\n"," 'pixel437': tensor(0.1490),\n"," 'pixel438': tensor(-0.1739),\n"," 'pixel439': tensor(0.3566),\n"," 'pixel440': tensor(0.3589),\n"," 'pixel441': tensor(0.2496),\n"," 'pixel442': tensor(0.1783),\n"," 'pixel443': tensor(-0.4975),\n"," 'pixel444': tensor(0.0603),\n"," 'pixel445': tensor(0.4014),\n"," 'pixel446': tensor(-0.3923),\n"," 'pixel447': tensor(0.4524),\n"," 'pixel448': tensor(-0.0623),\n"," 'pixel449': tensor(-0.4601),\n"," 'pixel450': tensor(-0.3948),\n"," 'pixel451': tensor(0.1046),\n"," 'pixel452': tensor(-0.1376),\n"," 'pixel453': tensor(-0.0412),\n"," 'pixel454': tensor(-0.1603),\n"," 'pixel455': tensor(-0.2451),\n"," 'pixel456': tensor(-0.1625),\n"," 'pixel457': tensor(0.0919),\n"," 'pixel458': tensor(0.0113),\n"," 'pixel459': tensor(-0.0148),\n"," 'pixel460': tensor(-0.0847),\n"," 'pixel461': tensor(0.0424),\n"," 'pixel462': tensor(-0.0909),\n"," 'pixel463': tensor(0.2276),\n"," 'pixel464': tensor(-0.2822),\n"," 'pixel465': tensor(0.2177),\n"," 'pixel466': tensor(-0.0592),\n"," 'pixel467': tensor(-0.3469),\n"," 'pixel468': tensor(0.4021),\n"," 'pixel469': tensor(0.3246),\n"," 'pixel470': tensor(0.2187),\n"," 'pixel471': tensor(-0.0623),\n"," 'pixel472': tensor(0.0305),\n"," 'pixel473': tensor(-0.0607),\n"," 'pixel474': tensor(0.0868),\n"," 'pixel475': tensor(0.1258),\n"," 'pixel476': tensor(0.0892),\n"," 'pixel477': tensor(0.1236),\n"," 'pixel478': tensor(-0.4507),\n"," 'pixel479': tensor(-0.2372),\n"," 'pixel480': tensor(0.4464),\n"," 'pixel481': tensor(-0.0040),\n"," 'pixel482': tensor(-0.3068),\n"," 'pixel483': tensor(0.1441),\n"," 'pixel484': tensor(-0.0835),\n"," 'pixel485': tensor(0.0747),\n"," 'pixel486': tensor(-0.1456),\n"," 'pixel487': tensor(-0.0087),\n"," 'pixel488': tensor(0.2573),\n"," 'pixel489': tensor(0.0483),\n"," 'pixel490': tensor(0.2435),\n"," 'pixel491': tensor(0.2839),\n"," 'pixel492': tensor(0.5851),\n"," 'pixel493': tensor(0.1452),\n"," 'pixel494': tensor(-0.1629),\n"," 'pixel495': tensor(0.3526),\n"," 'pixel496': tensor(-0.2308),\n"," 'pixel497': tensor(-0.4804),\n"," 'pixel498': tensor(-0.4981),\n"," 'pixel499': tensor(0.4374),\n"," 'pixel500': tensor(0.4705),\n"," 'pixel501': tensor(0.0174),\n"," 'pixel502': tensor(-0.2612),\n"," 'pixel503': tensor(-0.3491),\n"," 'pixel504': tensor(-0.2072),\n"," 'pixel505': tensor(0.2667),\n"," 'pixel506': tensor(-0.2693),\n"," 'pixel507': tensor(-0.0191),\n"," 'pixel508': tensor(-0.3274),\n"," 'pixel509': tensor(-0.1945),\n"," 'pixel510': tensor(-0.0123),\n"," 'pixel511': tensor(-0.0954),\n"," 'pixel512': tensor(0.1495),\n"," 'pixel513': tensor(0.0587),\n"," 'pixel514': tensor(-0.2390),\n"," 'pixel515': tensor(-0.2763),\n"," 'pixel516': tensor(-0.1056),\n"," 'pixel517': tensor(-0.3339),\n"," 'pixel518': tensor(0.0578),\n"," 'pixel519': tensor(0.4293),\n"," 'pixel520': tensor(-0.1161),\n"," 'pixel521': tensor(0.4205),\n"," 'pixel522': tensor(0.1114),\n"," 'pixel523': tensor(-0.0241),\n"," 'pixel524': tensor(-0.4958),\n"," 'pixel525': tensor(0.1154),\n"," 'pixel526': tensor(-0.2590),\n"," 'pixel527': tensor(0.1159),\n"," 'pixel528': tensor(-0.2624),\n"," 'pixel529': tensor(-0.3884),\n"," 'pixel530': tensor(0.1880),\n"," 'pixel531': tensor(0.2330),\n"," 'pixel532': tensor(-0.4890),\n"," 'pixel533': tensor(0.1850),\n"," 'pixel534': tensor(0.4498),\n"," 'pixel535': tensor(0.0343),\n"," 'pixel536': tensor(0.1801),\n"," 'pixel537': tensor(-0.0389),\n"," 'pixel538': tensor(-0.4283),\n"," 'pixel539': tensor(-0.3376),\n"," 'pixel540': tensor(0.1888),\n"," 'pixel541': tensor(0.1765),\n"," 'pixel542': tensor(-0.0545),\n"," 'pixel543': tensor(-0.0315),\n"," 'pixel544': tensor(0.0350),\n"," 'pixel545': tensor(-0.2243),\n"," 'pixel546': tensor(0.4634),\n"," 'pixel547': tensor(-0.0210),\n"," 'pixel548': tensor(0.1119),\n"," 'pixel549': tensor(0.3961),\n"," 'pixel550': tensor(-0.4428),\n"," 'pixel551': tensor(0.3658),\n"," 'pixel552': tensor(0.0022),\n"," 'pixel553': tensor(0.1596),\n"," 'pixel554': tensor(0.2087),\n"," 'pixel555': tensor(-0.2046),\n"," 'pixel556': tensor(-0.0099),\n"," 'pixel557': tensor(-0.2274),\n"," 'pixel558': tensor(0.1290),\n"," 'pixel559': tensor(-0.4438),\n"," 'pixel560': tensor(-0.1012),\n"," 'pixel561': tensor(0.0240),\n"," 'pixel562': tensor(-0.4145),\n"," 'pixel563': tensor(0.3821),\n"," 'pixel564': tensor(-0.3319),\n"," 'pixel565': tensor(0.3531),\n"," 'pixel566': tensor(-0.4282),\n"," 'pixel567': tensor(0.2800),\n"," 'pixel568': tensor(0.4102),\n"," 'pixel569': tensor(-0.0463),\n"," 'pixel570': tensor(0.3835),\n"," 'pixel571': tensor(0.4289),\n"," 'pixel572': tensor(0.2702),\n"," 'pixel573': tensor(0.1523),\n"," 'pixel574': tensor(0.4182),\n"," 'pixel575': tensor(-0.4933),\n"," 'pixel576': tensor(0.0174),\n"," 'pixel577': tensor(0.4513),\n"," 'pixel578': tensor(0.3543),\n"," 'pixel579': tensor(-0.4430),\n"," 'pixel580': tensor(0.2370),\n"," 'pixel581': tensor(-0.0605),\n"," 'pixel582': tensor(0.4500),\n"," 'pixel583': tensor(0.4380),\n"," 'pixel584': tensor(0.0304),\n"," 'pixel585': tensor(0.1888),\n"," 'pixel586': tensor(-0.2217),\n"," 'pixel587': tensor(0.3127),\n"," 'pixel588': tensor(-0.3777),\n"," 'pixel589': tensor(0.3264),\n"," 'pixel590': tensor(0.2226),\n"," 'pixel591': tensor(-0.3260),\n"," 'pixel592': tensor(-0.2379),\n"," 'pixel593': tensor(-0.4906),\n"," 'pixel594': tensor(-0.3468),\n"," 'pixel595': tensor(-0.4203),\n"," 'pixel596': tensor(-0.4606),\n"," 'pixel597': tensor(-0.3458),\n"," 'pixel598': tensor(0.3146),\n"," 'pixel599': tensor(-0.2426),\n"," 'pixel600': tensor(0.2405),\n"," 'pixel601': tensor(-0.1718),\n"," 'pixel602': tensor(0.1593),\n"," 'pixel603': tensor(0.0012),\n"," 'pixel604': tensor(-0.0751),\n"," 'pixel605': tensor(0.4619),\n"," 'pixel606': tensor(0.0956),\n"," 'pixel607': tensor(0.0719),\n"," 'pixel608': tensor(-0.0551),\n"," 'pixel609': tensor(-0.1215),\n"," 'pixel610': tensor(-0.2363),\n"," 'pixel611': tensor(-0.2204),\n"," 'pixel612': tensor(-0.1388),\n"," 'pixel613': tensor(-0.2341),\n"," 'pixel614': tensor(0.0793),\n"," 'pixel615': tensor(0.2478),\n"," 'pixel616': tensor(-0.4342),\n"," 'pixel617': tensor(0.0343),\n"," 'pixel618': tensor(-0.0659),\n"," 'pixel619': tensor(-0.0006),\n"," 'pixel620': tensor(-0.3783),\n"," 'pixel621': tensor(0.0983),\n"," 'pixel622': tensor(0.0784),\n"," 'pixel623': tensor(-0.0733),\n"," 'pixel624': tensor(-0.0621),\n"," 'pixel625': tensor(0.0240),\n"," 'pixel626': tensor(-0.2409),\n"," 'pixel627': tensor(0.3809),\n"," 'pixel628': tensor(-0.0083),\n"," 'pixel629': tensor(-0.3725),\n"," 'pixel630': tensor(0.2840),\n"," 'pixel631': tensor(-0.1364),\n"," 'pixel632': tensor(0.2740),\n"," 'pixel633': tensor(0.0964),\n"," 'pixel634': tensor(-0.1878),\n"," 'pixel635': tensor(0.2961),\n"," 'pixel636': tensor(-0.3206),\n"," 'pixel637': tensor(-0.3903),\n"," 'pixel638': tensor(-0.0685),\n"," 'pixel639': tensor(-0.3124),\n"," 'pixel640': tensor(-0.0052),\n"," 'pixel641': tensor(-0.3068),\n"," 'pixel642': tensor(0.4171),\n"," 'pixel643': tensor(-0.3776),\n"," 'pixel644': tensor(-0.1271),\n"," 'pixel645': tensor(0.1943),\n"," 'pixel646': tensor(-0.4005),\n"," 'pixel647': tensor(0.2555),\n"," 'pixel648': tensor(0.4458),\n"," 'pixel649': tensor(0.4598),\n"," 'pixel650': tensor(-0.3968),\n"," 'pixel651': tensor(-0.2856),\n"," 'pixel652': tensor(0.4038),\n"," 'pixel653': tensor(0.0911),\n"," 'pixel654': tensor(0.2153),\n"," 'pixel655': tensor(0.3065),\n"," 'pixel656': tensor(0.2453),\n"," 'pixel657': tensor(0.1707),\n"," 'pixel658': tensor(0.1150),\n"," 'pixel659': tensor(0.3384),\n"," 'pixel660': tensor(-0.4044),\n"," 'pixel661': tensor(0.1583),\n"," 'pixel662': tensor(0.1824),\n"," 'pixel663': tensor(-0.1926),\n"," 'pixel664': tensor(0.1240),\n"," 'pixel665': tensor(-0.1176),\n"," 'pixel666': tensor(-0.0935),\n"," 'pixel667': tensor(-0.3841),\n"," 'pixel668': tensor(-0.4653),\n"," 'pixel669': tensor(-0.1722),\n"," 'pixel670': tensor(0.3460),\n"," 'pixel671': tensor(0.0519),\n"," 'pixel672': tensor(-0.3962),\n"," 'pixel673': tensor(0.0602),\n"," 'pixel674': tensor(-0.1812),\n"," 'pixel675': tensor(-0.0735),\n"," 'pixel676': tensor(-0.3771),\n"," 'pixel677': tensor(0.2044),\n"," 'pixel678': tensor(0.3477),\n"," 'pixel679': tensor(-0.3624),\n"," 'pixel680': tensor(-0.2339),\n"," 'pixel681': tensor(-0.2268),\n"," 'pixel682': tensor(-0.0077),\n"," 'pixel683': tensor(0.1907),\n"," 'pixel684': tensor(0.4982),\n"," 'pixel685': tensor(0.0944),\n"," 'pixel686': tensor(0.5079),\n"," 'pixel687': tensor(-0.1116),\n"," 'pixel688': tensor(0.2504),\n"," 'pixel689': tensor(-0.3409),\n"," 'pixel690': tensor(0.2888),\n"," 'pixel691': tensor(0.4537),\n"," 'pixel692': tensor(-0.1749),\n"," 'pixel693': tensor(-0.1951),\n"," 'pixel694': tensor(0.1644),\n"," 'pixel695': tensor(-0.2857),\n"," 'pixel696': tensor(-0.0056),\n"," 'pixel697': tensor(-0.2126),\n"," 'pixel698': tensor(-0.3807),\n"," 'pixel699': tensor(-0.2650),\n"," 'pixel700': tensor(0.2225),\n"," 'pixel701': tensor(-0.3797),\n"," 'pixel702': tensor(0.1351),\n"," 'pixel703': tensor(0.2076),\n"," 'pixel704': tensor(0.3322),\n"," 'pixel705': tensor(0.3442),\n"," 'pixel706': tensor(0.2155),\n"," 'pixel707': tensor(-0.4239),\n"," 'pixel708': tensor(-0.2088),\n"," 'pixel709': tensor(0.0330),\n"," 'pixel710': tensor(0.4520),\n"," 'pixel711': tensor(-0.3006),\n"," 'pixel712': tensor(0.1934),\n"," 'pixel713': tensor(-0.3318),\n"," 'pixel714': tensor(0.5411),\n"," 'pixel715': tensor(-0.4293),\n"," 'pixel716': tensor(0.2177),\n"," 'pixel717': tensor(0.3127),\n"," 'pixel718': tensor(-0.1307),\n"," 'pixel719': tensor(-0.2105),\n"," 'pixel720': tensor(-0.2345),\n"," 'pixel721': tensor(    0.0000),\n"," 'pixel722': tensor(-0.4589),\n"," 'pixel723': tensor(0.3873),\n"," 'pixel724': tensor(0.1037),\n"," 'pixel725': tensor(-0.0864),\n"," 'pixel726': tensor(0.0629),\n"," 'pixel727': tensor(-0.4212),\n"," 'pixel728': tensor(-0.3595),\n"," 'pixel729': tensor(-0.0070),\n"," 'pixel730': tensor(-0.2864),\n"," 'pixel731': tensor(0.0273),\n"," 'pixel732': tensor(0.2374),\n"," 'pixel733': tensor(-0.0301),\n"," 'pixel734': tensor(0.0443),\n"," 'pixel735': tensor(0.2937),\n"," 'pixel736': tensor(0.3258),\n"," 'pixel737': tensor(-0.3951),\n"," 'pixel738': tensor(0.2097),\n"," 'pixel739': tensor(-0.1186),\n"," 'pixel740': tensor(-0.2566),\n"," 'pixel741': tensor(-0.2796),\n"," 'pixel742': tensor(0.2906),\n"," 'pixel743': tensor(-0.0441),\n"," 'pixel744': tensor(-0.4100),\n"," 'pixel745': tensor(0.5098),\n"," 'pixel746': tensor(0.3810),\n"," 'pixel747': tensor(0.3418),\n"," 'pixel748': tensor(0.3081),\n"," 'pixel749': tensor(-0.4869),\n"," 'pixel750': tensor(-0.4844),\n"," 'pixel751': tensor(0.2264),\n"," 'pixel752': tensor(0.2172),\n"," 'pixel753': tensor(0.0324),\n"," 'pixel754': tensor(-0.0769),\n"," 'pixel755': tensor(-0.1767),\n"," 'pixel756': tensor(-0.1453),\n"," 'pixel757': tensor(0.3159),\n"," 'pixel758': tensor(-0.0257),\n"," 'pixel759': tensor(0.1365),\n"," 'pixel760': tensor(-0.1752),\n"," 'pixel761': tensor(-0.0453),\n"," 'pixel762': tensor(0.1942),\n"," 'pixel763': tensor(0.2269),\n"," 'pixel764': tensor(0.3787),\n"," 'pixel765': tensor(0.2541),\n"," 'pixel766': tensor(0.0726),\n"," 'pixel767': tensor(-0.2560),\n"," 'pixel768': tensor(0.1559),\n"," 'pixel769': tensor(0.0830),\n"," 'pixel770': tensor(0.4474),\n"," 'pixel771': tensor(0.1566),\n"," 'pixel772': tensor(-0.0906),\n"," 'pixel773': tensor(-0.3273),\n"," 'pixel774': tensor(0.3020),\n"," 'pixel775': tensor(0.4703),\n"," 'pixel776': tensor(0.1230),\n"," 'pixel777': tensor(-0.0054),\n"," 'pixel778': tensor(-0.2684),\n"," 'pixel779': tensor(-0.4286),\n"," 'pixel780': tensor(-0.1200),\n"," 'pixel781': tensor(0.0068),\n"," 'pixel782': tensor(0.3211),\n"," 'pixel783': tensor(-0.3081)}"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["def show_coeffs(): return dict(zip(indep_vars.columns, coeffs.requires_grad_(False)))\n","show_coeffs()"]},{"cell_type":"markdown","metadata":{},"source":["## Measuring accuracy"]},{"cell_type":"markdown","metadata":{},"source":["The Kaggle competition is not, however, scored by absolute error (which is our loss function). It's scored by *accuracy* -- the proportion of rows where we correctly predict survival. Let's see how accurate we were on the validation set. First, calculate the predictions:"]},{"cell_type":"code","execution_count":97,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:36:30.826372Z","iopub.status.busy":"2022-05-30T22:36:30.825856Z","iopub.status.idle":"2022-05-30T22:36:30.831613Z","shell.execute_reply":"2022-05-30T22:36:30.830756Z","shell.execute_reply.started":"2022-05-30T22:36:30.826322Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([6, 3, 6, 1, 7, 9, 1,  ..., 3, 8, 3, 5, 4, 6, 3])\n"]},{"data":{"text/plain":["tensor([ 4.,  2.,  3.,  2.,  6.,  8.,  3.,  ..., -1.,  7.,  5.,  2.,  9.,  5.,  1.])"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["preds = calc_preds(coeffs, val_indep)\n","print(val_dep)\n","torch.round(preds)"]},{"cell_type":"markdown","metadata":{},"source":["We'll assume that any passenger with a score of over `0.5` is predicted to survive. So that means we're correct for each row where `preds>0.5` is the same as the dependent variable:"]},{"cell_type":"code","execution_count":96,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:36:33.618899Z","iopub.status.busy":"2022-05-30T22:36:33.618455Z","iopub.status.idle":"2022-05-30T22:36:33.62703Z","shell.execute_reply":"2022-05-30T22:36:33.625949Z","shell.execute_reply.started":"2022-05-30T22:36:33.618867Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([False, False, False, False, False, False, False, False, False,  True,  True, False, False, False, False, False])"]},"execution_count":96,"metadata":{},"output_type":"execute_result"}],"source":["results = torch.eq(val_dep, torch.round(preds))\n","results[:16]"]},{"cell_type":"markdown","metadata":{},"source":["Let's see what our average accuracy is:"]},{"cell_type":"code","execution_count":98,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:36:35.725637Z","iopub.status.busy":"2022-05-30T22:36:35.725112Z","iopub.status.idle":"2022-05-30T22:36:35.732969Z","shell.execute_reply":"2022-05-30T22:36:35.732241Z","shell.execute_reply.started":"2022-05-30T22:36:35.725599Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor(0.1673)"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["results.float().mean()"]},{"cell_type":"markdown","metadata":{},"source":["That's not a bad start at all! We'll create a function so we can calcuate the accuracy easy for other models we train:"]},{"cell_type":"code","execution_count":99,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:36:40.356505Z","iopub.status.busy":"2022-05-30T22:36:40.356043Z","iopub.status.idle":"2022-05-30T22:36:40.365187Z","shell.execute_reply":"2022-05-30T22:36:40.364153Z","shell.execute_reply.started":"2022-05-30T22:36:40.356471Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor(0.1673)"]},"execution_count":99,"metadata":{},"output_type":"execute_result"}],"source":["def acc(coeffs): return (torch.eq(val_dep, torch.round(calc_preds(coeffs, val_indep)))).float().mean()\n","acc(coeffs)"]},{"cell_type":"markdown","metadata":{},"source":["## Using sigmoid"]},{"cell_type":"markdown","metadata":{},"source":["Looking at our predictions, there's one obvious problem -- some of our predictions of the probability of survival are `>1`, and some are `<0`:"]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:36:42.344823Z","iopub.status.busy":"2022-05-30T22:36:42.344533Z","iopub.status.idle":"2022-05-30T22:36:42.352948Z","shell.execute_reply":"2022-05-30T22:36:42.351968Z","shell.execute_reply.started":"2022-05-30T22:36:42.344794Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([4.2534, 2.1610, 3.4290, 1.8156, 5.9058, 7.5189, 3.1806, 3.4630, 4.5087, 5.8910, 5.4332, 6.4932, 0.6244, 5.2409, 1.7034, 5.3738,\n","        3.7018, 5.2964, 3.5246, 0.8176, 6.1915, 2.9467, 2.4050, 6.3866, 7.0920, 7.9649, 7.4542, 5.4504])"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["preds[:28]"]},{"cell_type":"markdown","metadata":{},"source":["To fix this, we should pass every prediction through the *sigmoid function*, which has a minimum at zero and maximum at one, and is defined as follows:"]},{"cell_type":"code","execution_count":103,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:36:44.914015Z","iopub.status.busy":"2022-05-30T22:36:44.913101Z","iopub.status.idle":"2022-05-30T22:36:46.311818Z","shell.execute_reply":"2022-05-30T22:36:46.311008Z","shell.execute_reply.started":"2022-05-30T22:36:44.913968Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import sympy\n","sympy.plot(\"9/(1+exp(-x))\", xlim=(-5,5));"]},{"cell_type":"markdown","metadata":{},"source":["PyTorch already defines that function for us, so we can modify `calc_preds` to use it:"]},{"cell_type":"code","execution_count":104,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:36:46.313644Z","iopub.status.busy":"2022-05-30T22:36:46.313435Z","iopub.status.idle":"2022-05-30T22:36:46.317749Z","shell.execute_reply":"2022-05-30T22:36:46.3169Z","shell.execute_reply.started":"2022-05-30T22:36:46.313618Z"},"trusted":true},"outputs":[],"source":["def calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1))*9"]},{"cell_type":"markdown","metadata":{},"source":["Let's train a new model now, using this updated function to calculate predictions:"]},{"cell_type":"code","execution_count":106,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:23.22576Z","iopub.status.busy":"2022-05-30T22:38:23.225051Z","iopub.status.idle":"2022-05-30T22:38:23.250206Z","shell.execute_reply":"2022-05-30T22:38:23.249321Z","shell.execute_reply.started":"2022-05-30T22:38:23.225722Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.956; 2.858; 2.771; 2.689; 2.611; 2.538; 2.469; 2.406; 2.347; 2.295; 2.248; 2.206; 2.169; 2.136; 2.107; 2.082; 2.059; 2.038; 2.020; 2.003; 1.988; 1.975; 1.962; 1.950; 1.940; 1.930; 1.920; 1.911; 1.903; 1.895; "]}],"source":["coeffs = train_model(lr=0.1)"]},{"cell_type":"markdown","metadata":{},"source":["The loss has improved by a lot. Let's check the accuracy:"]},{"cell_type":"code","execution_count":107,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:28.519132Z","iopub.status.busy":"2022-05-30T22:38:28.518642Z","iopub.status.idle":"2022-05-30T22:38:28.527145Z","shell.execute_reply":"2022-05-30T22:38:28.526248Z","shell.execute_reply.started":"2022-05-30T22:38:28.519078Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor(0.2426)"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["acc(coeffs)"]},{"cell_type":"markdown","metadata":{},"source":["That's improved too! Here's the coefficients of our trained model:"]},{"cell_type":"code","execution_count":108,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:32.01697Z","iopub.status.busy":"2022-05-30T22:38:32.015953Z","iopub.status.idle":"2022-05-30T22:38:32.02724Z","shell.execute_reply":"2022-05-30T22:38:32.026125Z","shell.execute_reply.started":"2022-05-30T22:38:32.016924Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'pixel0': tensor(-0.4629),\n"," 'pixel1': tensor(0.1386),\n"," 'pixel2': tensor(0.2409),\n"," 'pixel3': tensor(-0.2262),\n"," 'pixel4': tensor(-0.2632),\n"," 'pixel5': tensor(-0.3147),\n"," 'pixel6': tensor(0.4876),\n"," 'pixel7': tensor(0.3136),\n"," 'pixel8': tensor(0.2799),\n"," 'pixel9': tensor(-0.4392),\n"," 'pixel10': tensor(0.2103),\n"," 'pixel11': tensor(0.3625),\n"," 'pixel12': tensor(0.1724),\n"," 'pixel13': tensor(0.2326),\n"," 'pixel14': tensor(-0.3575),\n"," 'pixel15': tensor(-0.0010),\n"," 'pixel16': tensor(-0.1833),\n"," 'pixel17': tensor(-0.2411),\n"," 'pixel18': tensor(0.0489),\n"," 'pixel19': tensor(0.0866),\n"," 'pixel20': tensor(-0.0534),\n"," 'pixel21': tensor(0.3132),\n"," 'pixel22': tensor(-0.1487),\n"," 'pixel23': tensor(-0.2551),\n"," 'pixel24': tensor(0.3328),\n"," 'pixel25': tensor(0.1292),\n"," 'pixel26': tensor(0.2361),\n"," 'pixel27': tensor(-0.2622),\n"," 'pixel28': tensor(0.4051),\n"," 'pixel29': tensor(-0.2674),\n"," 'pixel30': tensor(-0.2312),\n"," 'pixel31': tensor(0.1147),\n"," 'pixel32': tensor(0.4072),\n"," 'pixel33': tensor(0.2834),\n"," 'pixel34': tensor(0.0837),\n"," 'pixel35': tensor(-0.0502),\n"," 'pixel36': tensor(0.4356),\n"," 'pixel37': tensor(-0.3402),\n"," 'pixel38': tensor(-0.4532),\n"," 'pixel39': tensor(0.1472),\n"," 'pixel40': tensor(-0.4628),\n"," 'pixel41': tensor(-0.3019),\n"," 'pixel42': tensor(-0.1281),\n"," 'pixel43': tensor(-0.0846),\n"," 'pixel44': tensor(0.4676),\n"," 'pixel45': tensor(0.3964),\n"," 'pixel46': tensor(-0.4853),\n"," 'pixel47': tensor(0.2098),\n"," 'pixel48': tensor(-0.3018),\n"," 'pixel49': tensor(0.4186),\n"," 'pixel50': tensor(0.1091),\n"," 'pixel51': tensor(0.0792),\n"," 'pixel52': tensor(-0.1708),\n"," 'pixel53': tensor(-0.1882),\n"," 'pixel54': tensor(0.4861),\n"," 'pixel55': tensor(-0.2534),\n"," 'pixel56': tensor(-0.2502),\n"," 'pixel57': tensor(-0.0770),\n"," 'pixel58': tensor(0.3336),\n"," 'pixel59': tensor(-0.4977),\n"," 'pixel60': tensor(-0.2559),\n"," 'pixel61': tensor(0.0891),\n"," 'pixel62': tensor(-0.4666),\n"," 'pixel63': tensor(0.3730),\n"," 'pixel64': tensor(-0.1084),\n"," 'pixel65': tensor(0.3485),\n"," 'pixel66': tensor(-0.3367),\n"," 'pixel67': tensor(0.1198),\n"," 'pixel68': tensor(-0.2765),\n"," 'pixel69': tensor(0.0339),\n"," 'pixel70': tensor(-0.1572),\n"," 'pixel71': tensor(-0.0864),\n"," 'pixel72': tensor(-0.1413),\n"," 'pixel73': tensor(0.3573),\n"," 'pixel74': tensor(0.3668),\n"," 'pixel75': tensor(0.4788),\n"," 'pixel76': tensor(-0.0264),\n"," 'pixel77': tensor(-0.3020),\n"," 'pixel78': tensor(0.1481),\n"," 'pixel79': tensor(0.3739),\n"," 'pixel80': tensor(0.1068),\n"," 'pixel81': tensor(-0.1042),\n"," 'pixel82': tensor(-0.4297),\n"," 'pixel83': tensor(-0.0069),\n"," 'pixel84': tensor(-0.2685),\n"," 'pixel85': tensor(0.2132),\n"," 'pixel86': tensor(-0.0946),\n"," 'pixel87': tensor(-0.4803),\n"," 'pixel88': tensor(0.1939),\n"," 'pixel89': tensor(0.0920),\n"," 'pixel90': tensor(0.2915),\n"," 'pixel91': tensor(-0.2698),\n"," 'pixel92': tensor(0.4453),\n"," 'pixel93': tensor(-0.2967),\n"," 'pixel94': tensor(0.1641),\n"," 'pixel95': tensor(0.3259),\n"," 'pixel96': tensor(-0.1422),\n"," 'pixel97': tensor(0.2934),\n"," 'pixel98': tensor(0.0595),\n"," 'pixel99': tensor(0.2475),\n"," 'pixel100': tensor(0.2089),\n"," 'pixel101': tensor(-0.2885),\n"," 'pixel102': tensor(-0.3160),\n"," 'pixel103': tensor(-0.3427),\n"," 'pixel104': tensor(0.1303),\n"," 'pixel105': tensor(-0.2744),\n"," 'pixel106': tensor(-0.3781),\n"," 'pixel107': tensor(-0.3079),\n"," 'pixel108': tensor(-0.1848),\n"," 'pixel109': tensor(0.3502),\n"," 'pixel110': tensor(-0.4067),\n"," 'pixel111': tensor(-0.3355),\n"," 'pixel112': tensor(-0.4195),\n"," 'pixel113': tensor(0.4183),\n"," 'pixel114': tensor(0.0745),\n"," 'pixel115': tensor(-0.2617),\n"," 'pixel116': tensor(0.1317),\n"," 'pixel117': tensor(0.1040),\n"," 'pixel118': tensor(-0.1730),\n"," 'pixel119': tensor(0.2847),\n"," 'pixel120': tensor(0.1230),\n"," 'pixel121': tensor(-0.1235),\n"," 'pixel122': tensor(-0.3590),\n"," 'pixel123': tensor(-0.1890),\n"," 'pixel124': tensor(0.2778),\n"," 'pixel125': tensor(-0.3605),\n"," 'pixel126': tensor(-0.0580),\n"," 'pixel127': tensor(0.0395),\n"," 'pixel128': tensor(-0.2425),\n"," 'pixel129': tensor(0.3795),\n"," 'pixel130': tensor(-0.0156),\n"," 'pixel131': tensor(-0.2507),\n"," 'pixel132': tensor(-0.3630),\n"," 'pixel133': tensor(0.0831),\n"," 'pixel134': tensor(0.0137),\n"," 'pixel135': tensor(0.2877),\n"," 'pixel136': tensor(0.0116),\n"," 'pixel137': tensor(0.2867),\n"," 'pixel138': tensor(-0.2501),\n"," 'pixel139': tensor(-0.3194),\n"," 'pixel140': tensor(-0.0428),\n"," 'pixel141': tensor(0.2932),\n"," 'pixel142': tensor(-0.1903),\n"," 'pixel143': tensor(-0.4143),\n"," 'pixel144': tensor(-0.1949),\n"," 'pixel145': tensor(-0.1338),\n"," 'pixel146': tensor(-0.1390),\n"," 'pixel147': tensor(-0.0900),\n"," 'pixel148': tensor(-0.3320),\n"," 'pixel149': tensor(0.0876),\n"," 'pixel150': tensor(-0.2655),\n"," 'pixel151': tensor(0.1648),\n"," 'pixel152': tensor(-0.2541),\n"," 'pixel153': tensor(-0.0434),\n"," 'pixel154': tensor(-0.3472),\n"," 'pixel155': tensor(-0.1666),\n"," 'pixel156': tensor(0.2455),\n"," 'pixel157': tensor(-0.3609),\n"," 'pixel158': tensor(-0.1046),\n"," 'pixel159': tensor(-0.1066),\n"," 'pixel160': tensor(-0.0406),\n"," 'pixel161': tensor(0.4156),\n"," 'pixel162': tensor(-0.3319),\n"," 'pixel163': tensor(0.4594),\n"," 'pixel164': tensor(0.4945),\n"," 'pixel165': tensor(0.0702),\n"," 'pixel166': tensor(0.4646),\n"," 'pixel167': tensor(0.3530),\n"," 'pixel168': tensor(0.0762),\n"," 'pixel169': tensor(0.1317),\n"," 'pixel170': tensor(0.1661),\n"," 'pixel171': tensor(0.0825),\n"," 'pixel172': tensor(-0.4608),\n"," 'pixel173': tensor(-0.3935),\n"," 'pixel174': tensor(-0.3646),\n"," 'pixel175': tensor(-0.0907),\n"," 'pixel176': tensor(-0.2064),\n"," 'pixel177': tensor(0.2443),\n"," 'pixel178': tensor(-0.4270),\n"," 'pixel179': tensor(-0.4521),\n"," 'pixel180': tensor(0.3248),\n"," 'pixel181': tensor(-0.4770),\n"," 'pixel182': tensor(0.0953),\n"," 'pixel183': tensor(0.2600),\n"," 'pixel184': tensor(-0.1628),\n"," 'pixel185': tensor(-0.2171),\n"," 'pixel186': tensor(-0.0083),\n"," 'pixel187': tensor(-0.4437),\n"," 'pixel188': tensor(0.3485),\n"," 'pixel189': tensor(-0.2116),\n"," 'pixel190': tensor(-0.0618),\n"," 'pixel191': tensor(-0.2766),\n"," 'pixel192': tensor(-0.1995),\n"," 'pixel193': tensor(0.4613),\n"," 'pixel194': tensor(-0.1493),\n"," 'pixel195': tensor(-0.3909),\n"," 'pixel196': tensor(0.3088),\n"," 'pixel197': tensor(-0.0719),\n"," 'pixel198': tensor(-0.4053),\n"," 'pixel199': tensor(0.3430),\n"," 'pixel200': tensor(0.1548),\n"," 'pixel201': tensor(0.0184),\n"," 'pixel202': tensor(-0.4778),\n"," 'pixel203': tensor(0.2868),\n"," 'pixel204': tensor(0.0843),\n"," 'pixel205': tensor(0.1146),\n"," 'pixel206': tensor(-0.1680),\n"," 'pixel207': tensor(0.0829),\n"," 'pixel208': tensor(-0.3910),\n"," 'pixel209': tensor(0.2988),\n"," 'pixel210': tensor(0.2496),\n"," 'pixel211': tensor(0.1293),\n"," 'pixel212': tensor(-0.2454),\n"," 'pixel213': tensor(-0.3354),\n"," 'pixel214': tensor(-0.2324),\n"," 'pixel215': tensor(0.4255),\n"," 'pixel216': tensor(-0.0802),\n"," 'pixel217': tensor(-0.4793),\n"," 'pixel218': tensor(0.3386),\n"," 'pixel219': tensor(-0.3847),\n"," 'pixel220': tensor(-0.3839),\n"," 'pixel221': tensor(0.2222),\n"," 'pixel222': tensor(-0.4304),\n"," 'pixel223': tensor(-0.4856),\n"," 'pixel224': tensor(0.0426),\n"," 'pixel225': tensor(-0.0676),\n"," 'pixel226': tensor(-0.1168),\n"," 'pixel227': tensor(0.2731),\n"," 'pixel228': tensor(-0.4424),\n"," 'pixel229': tensor(-0.2620),\n"," 'pixel230': tensor(0.2079),\n"," 'pixel231': tensor(0.3498),\n"," 'pixel232': tensor(-0.3489),\n"," 'pixel233': tensor(-0.2182),\n"," 'pixel234': tensor(-0.3033),\n"," 'pixel235': tensor(0.1665),\n"," 'pixel236': tensor(0.3212),\n"," 'pixel237': tensor(-0.3925),\n"," 'pixel238': tensor(0.3069),\n"," 'pixel239': tensor(-0.0870),\n"," 'pixel240': tensor(0.3239),\n"," 'pixel241': tensor(0.1152),\n"," 'pixel242': tensor(-0.0566),\n"," 'pixel243': tensor(-0.2993),\n"," 'pixel244': tensor(-0.4850),\n"," 'pixel245': tensor(-0.1430),\n"," 'pixel246': tensor(0.1560),\n"," 'pixel247': tensor(0.0544),\n"," 'pixel248': tensor(0.4040),\n"," 'pixel249': tensor(0.0539),\n"," 'pixel250': tensor(-0.1029),\n"," 'pixel251': tensor(-0.3456),\n"," 'pixel252': tensor(-0.2749),\n"," 'pixel253': tensor(0.1889),\n"," 'pixel254': tensor(-0.3452),\n"," 'pixel255': tensor(-0.2616),\n"," 'pixel256': tensor(0.4055),\n"," 'pixel257': tensor(0.4126),\n"," 'pixel258': tensor(0.3351),\n"," 'pixel259': tensor(-0.4238),\n"," 'pixel260': tensor(0.3295),\n"," 'pixel261': tensor(-0.2617),\n"," 'pixel262': tensor(0.4826),\n"," 'pixel263': tensor(-0.4265),\n"," 'pixel264': tensor(-0.0897),\n"," 'pixel265': tensor(0.0724),\n"," 'pixel266': tensor(0.0316),\n"," 'pixel267': tensor(0.0523),\n"," 'pixel268': tensor(0.0754),\n"," 'pixel269': tensor(-0.4187),\n"," 'pixel270': tensor(0.0175),\n"," 'pixel271': tensor(-0.2768),\n"," 'pixel272': tensor(0.4135),\n"," 'pixel273': tensor(0.2920),\n"," 'pixel274': tensor(0.2566),\n"," 'pixel275': tensor(0.4824),\n"," 'pixel276': tensor(-0.3712),\n"," 'pixel277': tensor(0.3801),\n"," 'pixel278': tensor(-0.1324),\n"," 'pixel279': tensor(0.0470),\n"," 'pixel280': tensor(0.0853),\n"," 'pixel281': tensor(0.2161),\n"," 'pixel282': tensor(-0.2445),\n"," 'pixel283': tensor(-0.4590),\n"," 'pixel284': tensor(0.3033),\n"," 'pixel285': tensor(0.4053),\n"," 'pixel286': tensor(0.0079),\n"," 'pixel287': tensor(-0.4171),\n"," 'pixel288': tensor(0.0654),\n"," 'pixel289': tensor(0.4360),\n"," 'pixel290': tensor(0.0193),\n"," 'pixel291': tensor(0.4562),\n"," 'pixel292': tensor(-0.2608),\n"," 'pixel293': tensor(0.3035),\n"," 'pixel294': tensor(-0.4643),\n"," 'pixel295': tensor(-0.3996),\n"," 'pixel296': tensor(-0.3668),\n"," 'pixel297': tensor(-0.1761),\n"," 'pixel298': tensor(0.3260),\n"," 'pixel299': tensor(0.4356),\n"," 'pixel300': tensor(-0.1296),\n"," 'pixel301': tensor(-0.3011),\n"," 'pixel302': tensor(0.4004),\n"," 'pixel303': tensor(0.1506),\n"," 'pixel304': tensor(-0.1739),\n"," 'pixel305': tensor(-0.1000),\n"," 'pixel306': tensor(0.0485),\n"," 'pixel307': tensor(-0.2266),\n"," 'pixel308': tensor(-0.3955),\n"," 'pixel309': tensor(-0.1489),\n"," 'pixel310': tensor(0.3901),\n"," 'pixel311': tensor(-0.4646),\n"," 'pixel312': tensor(0.2558),\n"," 'pixel313': tensor(-0.1436),\n"," 'pixel314': tensor(-0.2549),\n"," 'pixel315': tensor(-0.0158),\n"," 'pixel316': tensor(0.2048),\n"," 'pixel317': tensor(-0.1006),\n"," 'pixel318': tensor(-0.3198),\n"," 'pixel319': tensor(0.1807),\n"," 'pixel320': tensor(0.1287),\n"," 'pixel321': tensor(-0.4051),\n"," 'pixel322': tensor(-0.4611),\n"," 'pixel323': tensor(0.0773),\n"," 'pixel324': tensor(0.0639),\n"," 'pixel325': tensor(-0.1349),\n"," 'pixel326': tensor(-0.1830),\n"," 'pixel327': tensor(0.3785),\n"," 'pixel328': tensor(0.4308),\n"," 'pixel329': tensor(-0.4641),\n"," 'pixel330': tensor(-0.3012),\n"," 'pixel331': tensor(-0.1673),\n"," 'pixel332': tensor(0.1174),\n"," 'pixel333': tensor(0.4057),\n"," 'pixel334': tensor(0.4521),\n"," 'pixel335': tensor(-0.3047),\n"," 'pixel336': tensor(0.2061),\n"," 'pixel337': tensor(-0.3025),\n"," 'pixel338': tensor(-0.2599),\n"," 'pixel339': tensor(0.3962),\n"," 'pixel340': tensor(-0.3834),\n"," 'pixel341': tensor(-0.1250),\n"," 'pixel342': tensor(-0.0092),\n"," 'pixel343': tensor(0.4599),\n"," 'pixel344': tensor(0.2312),\n"," 'pixel345': tensor(0.1618),\n"," 'pixel346': tensor(-0.0691),\n"," 'pixel347': tensor(-0.0876),\n"," 'pixel348': tensor(-0.0744),\n"," 'pixel349': tensor(0.1462),\n"," 'pixel350': tensor(-0.4847),\n"," 'pixel351': tensor(0.2299),\n"," 'pixel352': tensor(0.3761),\n"," 'pixel353': tensor(0.5007),\n"," 'pixel354': tensor(-0.2491),\n"," 'pixel355': tensor(0.0525),\n"," 'pixel356': tensor(0.1556),\n"," 'pixel357': tensor(0.0909),\n"," 'pixel358': tensor(-0.3739),\n"," 'pixel359': tensor(0.3384),\n"," 'pixel360': tensor(-0.4681),\n"," 'pixel361': tensor(-0.3858),\n"," 'pixel362': tensor(-0.0916),\n"," 'pixel363': tensor(-0.2867),\n"," 'pixel364': tensor(0.2480),\n"," 'pixel365': tensor(-0.2722),\n"," 'pixel366': tensor(-0.1471),\n"," 'pixel367': tensor(-0.0956),\n"," 'pixel368': tensor(0.4775),\n"," 'pixel369': tensor(0.3679),\n"," 'pixel370': tensor(-0.2625),\n"," 'pixel371': tensor(-0.2477),\n"," 'pixel372': tensor(0.0338),\n"," 'pixel373': tensor(0.2416),\n"," 'pixel374': tensor(0.1834),\n"," 'pixel375': tensor(0.4763),\n"," 'pixel376': tensor(-0.0462),\n"," 'pixel377': tensor(0.0840),\n"," 'pixel378': tensor(-0.3745),\n"," 'pixel379': tensor(0.0243),\n"," 'pixel380': tensor(0.5319),\n"," 'pixel381': tensor(0.0346),\n"," 'pixel382': tensor(-0.3325),\n"," 'pixel383': tensor(-0.0099),\n"," 'pixel384': tensor(-0.1422),\n"," 'pixel385': tensor(-0.5172),\n"," 'pixel386': tensor(0.1126),\n"," 'pixel387': tensor(-0.0994),\n"," 'pixel388': tensor(0.4063),\n"," 'pixel389': tensor(0.0905),\n"," 'pixel390': tensor(-0.3155),\n"," 'pixel391': tensor(-0.4252),\n"," 'pixel392': tensor(-0.1012),\n"," 'pixel393': tensor(-0.0952),\n"," 'pixel394': tensor(-0.1274),\n"," 'pixel395': tensor(0.3094),\n"," 'pixel396': tensor(0.4903),\n"," 'pixel397': tensor(0.0117),\n"," 'pixel398': tensor(0.1511),\n"," 'pixel399': tensor(0.2012),\n"," 'pixel400': tensor(-0.4752),\n"," 'pixel401': tensor(-0.2308),\n"," 'pixel402': tensor(-0.1271),\n"," 'pixel403': tensor(0.4214),\n"," 'pixel404': tensor(0.4484),\n"," 'pixel405': tensor(-0.1922),\n"," 'pixel406': tensor(0.0988),\n"," 'pixel407': tensor(-0.3020),\n"," 'pixel408': tensor(0.3021),\n"," 'pixel409': tensor(0.2938),\n"," 'pixel410': tensor(0.0795),\n"," 'pixel411': tensor(0.1322),\n"," 'pixel412': tensor(0.0405),\n"," 'pixel413': tensor(0.0011),\n"," 'pixel414': tensor(0.3140),\n"," 'pixel415': tensor(-0.4785),\n"," 'pixel416': tensor(-0.3084),\n"," 'pixel417': tensor(0.1248),\n"," 'pixel418': tensor(0.1583),\n"," 'pixel419': tensor(0.3701),\n"," 'pixel420': tensor(0.0802),\n"," 'pixel421': tensor(0.1926),\n"," 'pixel422': tensor(-0.2711),\n"," 'pixel423': tensor(-0.4395),\n"," 'pixel424': tensor(0.0639),\n"," 'pixel425': tensor(-0.5040),\n"," 'pixel426': tensor(-0.2906),\n"," 'pixel427': tensor(-0.1395),\n"," 'pixel428': tensor(0.2547),\n"," 'pixel429': tensor(-0.3332),\n"," 'pixel430': tensor(0.0202),\n"," 'pixel431': tensor(0.0946),\n"," 'pixel432': tensor(-0.1013),\n"," 'pixel433': tensor(0.4844),\n"," 'pixel434': tensor(-0.2158),\n"," 'pixel435': tensor(-0.3463),\n"," 'pixel436': tensor(0.4205),\n"," 'pixel437': tensor(0.0820),\n"," 'pixel438': tensor(-0.2325),\n"," 'pixel439': tensor(0.2895),\n"," 'pixel440': tensor(0.2823),\n"," 'pixel441': tensor(0.1937),\n"," 'pixel442': tensor(0.1552),\n"," 'pixel443': tensor(-0.5042),\n"," 'pixel444': tensor(0.0561),\n"," 'pixel445': tensor(0.4000),\n"," 'pixel446': tensor(-0.3932),\n"," 'pixel447': tensor(0.4522),\n"," 'pixel448': tensor(-0.0623),\n"," 'pixel449': tensor(-0.4602),\n"," 'pixel450': tensor(-0.3950),\n"," 'pixel451': tensor(0.1037),\n"," 'pixel452': tensor(-0.1447),\n"," 'pixel453': tensor(-0.0643),\n"," 'pixel454': tensor(-0.1985),\n"," 'pixel455': tensor(-0.3026),\n"," 'pixel456': tensor(-0.2201),\n"," 'pixel457': tensor(0.0572),\n"," 'pixel458': tensor(0.0093),\n"," 'pixel459': tensor(0.0083),\n"," 'pixel460': tensor(-0.0770),\n"," 'pixel461': tensor(-0.0104),\n"," 'pixel462': tensor(-0.1767),\n"," 'pixel463': tensor(0.1479),\n"," 'pixel464': tensor(-0.3518),\n"," 'pixel465': tensor(0.1501),\n"," 'pixel466': tensor(-0.1179),\n"," 'pixel467': tensor(-0.4096),\n"," 'pixel468': tensor(0.3357),\n"," 'pixel469': tensor(0.2810),\n"," 'pixel470': tensor(0.2026),\n"," 'pixel471': tensor(-0.0660),\n"," 'pixel472': tensor(0.0278),\n"," 'pixel473': tensor(-0.0613),\n"," 'pixel474': tensor(0.0854),\n"," 'pixel475': tensor(0.1255),\n"," 'pixel476': tensor(0.0892),\n"," 'pixel477': tensor(0.1235),\n"," 'pixel478': tensor(-0.4511),\n"," 'pixel479': tensor(-0.2376),\n"," 'pixel480': tensor(0.4392),\n"," 'pixel481': tensor(-0.0246),\n"," 'pixel482': tensor(-0.3408),\n"," 'pixel483': tensor(0.0960),\n"," 'pixel484': tensor(-0.1198),\n"," 'pixel485': tensor(0.0826),\n"," 'pixel486': tensor(-0.0886),\n"," 'pixel487': tensor(0.0566),\n"," 'pixel488': tensor(0.2823),\n"," 'pixel489': tensor(0.0069),\n"," 'pixel490': tensor(0.1430),\n"," 'pixel491': tensor(0.1697),\n"," 'pixel492': tensor(0.4950),\n"," 'pixel493': tensor(0.0716),\n"," 'pixel494': tensor(-0.2189),\n"," 'pixel495': tensor(0.3062),\n"," 'pixel496': tensor(-0.2633),\n"," 'pixel497': tensor(-0.4959),\n"," 'pixel498': tensor(-0.5035),\n"," 'pixel499': tensor(0.4364),\n"," 'pixel500': tensor(0.4689),\n"," 'pixel501': tensor(0.0161),\n"," 'pixel502': tensor(-0.2625),\n"," 'pixel503': tensor(-0.3496),\n"," 'pixel504': tensor(-0.2071),\n"," 'pixel505': tensor(0.2667),\n"," 'pixel506': tensor(-0.2693),\n"," 'pixel507': tensor(-0.0187),\n"," 'pixel508': tensor(-0.3322),\n"," 'pixel509': tensor(-0.2059),\n"," 'pixel510': tensor(-0.0374),\n"," 'pixel511': tensor(-0.1322),\n"," 'pixel512': tensor(0.1294),\n"," 'pixel513': tensor(0.0878),\n"," 'pixel514': tensor(-0.1686),\n"," 'pixel515': tensor(-0.2141),\n"," 'pixel516': tensor(-0.0975),\n"," 'pixel517': tensor(-0.3940),\n"," 'pixel518': tensor(-0.0571),\n"," 'pixel519': tensor(0.3104),\n"," 'pixel520': tensor(-0.2011),\n"," 'pixel521': tensor(0.3428),\n"," 'pixel522': tensor(0.0562),\n"," 'pixel523': tensor(-0.0534),\n"," 'pixel524': tensor(-0.5018),\n"," 'pixel525': tensor(0.1177),\n"," 'pixel526': tensor(-0.2579),\n"," 'pixel527': tensor(0.1167),\n"," 'pixel528': tensor(-0.2624),\n"," 'pixel529': tensor(-0.3881),\n"," 'pixel530': tensor(0.1876),\n"," 'pixel531': tensor(0.2329),\n"," 'pixel532': tensor(-0.4890),\n"," 'pixel533': tensor(0.1850),\n"," 'pixel534': tensor(0.4495),\n"," 'pixel535': tensor(0.0337),\n"," 'pixel536': tensor(0.1770),\n"," 'pixel537': tensor(-0.0431),\n"," 'pixel538': tensor(-0.4391),\n"," 'pixel539': tensor(-0.3595),\n"," 'pixel540': tensor(0.1686),\n"," 'pixel541': tensor(0.1811),\n"," 'pixel542': tensor(-0.0310),\n"," 'pixel543': tensor(-0.0334),\n"," 'pixel544': tensor(-0.0199),\n"," 'pixel545': tensor(-0.3259),\n"," 'pixel546': tensor(0.3281),\n"," 'pixel547': tensor(-0.1251),\n"," 'pixel548': tensor(0.0395),\n"," 'pixel549': tensor(0.3156),\n"," 'pixel550': tensor(-0.4922),\n"," 'pixel551': tensor(0.3477),\n"," 'pixel552': tensor(0.0039),\n"," 'pixel553': tensor(0.1622),\n"," 'pixel554': tensor(0.2085),\n"," 'pixel555': tensor(-0.2049),\n"," 'pixel556': tensor(-0.0117),\n"," 'pixel557': tensor(-0.2286),\n"," 'pixel558': tensor(0.1284),\n"," 'pixel559': tensor(-0.4438),\n"," 'pixel560': tensor(-0.1012),\n"," 'pixel561': tensor(0.0240),\n"," 'pixel562': tensor(-0.4151),\n"," 'pixel563': tensor(0.3806),\n"," 'pixel564': tensor(-0.3343),\n"," 'pixel565': tensor(0.3556),\n"," 'pixel566': tensor(-0.4252),\n"," 'pixel567': tensor(0.2758),\n"," 'pixel568': tensor(0.3963),\n"," 'pixel569': tensor(-0.0628),\n"," 'pixel570': tensor(0.3530),\n"," 'pixel571': tensor(0.3653),\n"," 'pixel572': tensor(0.1680),\n"," 'pixel573': tensor(0.0277),\n"," 'pixel574': tensor(0.2889),\n"," 'pixel575': tensor(-0.5744),\n"," 'pixel576': tensor(-0.0538),\n"," 'pixel577': tensor(0.3693),\n"," 'pixel578': tensor(0.3115),\n"," 'pixel579': tensor(-0.4472),\n"," 'pixel580': tensor(0.2382),\n"," 'pixel581': tensor(-0.0609),\n"," 'pixel582': tensor(0.4489),\n"," 'pixel583': tensor(0.4360),\n"," 'pixel584': tensor(0.0276),\n"," 'pixel585': tensor(0.1877),\n"," 'pixel586': tensor(-0.2221),\n"," 'pixel587': tensor(0.3128),\n"," 'pixel588': tensor(-0.3777),\n"," 'pixel589': tensor(0.3264),\n"," 'pixel590': tensor(0.2217),\n"," 'pixel591': tensor(-0.3279),\n"," 'pixel592': tensor(-0.2395),\n"," 'pixel593': tensor(-0.4856),\n"," 'pixel594': tensor(-0.3333),\n"," 'pixel595': tensor(-0.4066),\n"," 'pixel596': tensor(-0.4586),\n"," 'pixel597': tensor(-0.3613),\n"," 'pixel598': tensor(0.2703),\n"," 'pixel599': tensor(-0.3141),\n"," 'pixel600': tensor(0.1412),\n"," 'pixel601': tensor(-0.2827),\n"," 'pixel602': tensor(0.0445),\n"," 'pixel603': tensor(-0.0833),\n"," 'pixel604': tensor(-0.1557),\n"," 'pixel605': tensor(0.3887),\n"," 'pixel606': tensor(0.0647),\n"," 'pixel607': tensor(0.0716),\n"," 'pixel608': tensor(-0.0508),\n"," 'pixel609': tensor(-0.1157),\n"," 'pixel610': tensor(-0.2325),\n"," 'pixel611': tensor(-0.2202),\n"," 'pixel612': tensor(-0.1387),\n"," 'pixel613': tensor(-0.2346),\n"," 'pixel614': tensor(0.0790),\n"," 'pixel615': tensor(0.2478),\n"," 'pixel616': tensor(-0.4342),\n"," 'pixel617': tensor(0.0343),\n"," 'pixel618': tensor(-0.0662),\n"," 'pixel619': tensor(-0.0009),\n"," 'pixel620': tensor(-0.3801),\n"," 'pixel621': tensor(0.0997),\n"," 'pixel622': tensor(0.0914),\n"," 'pixel623': tensor(-0.0556),\n"," 'pixel624': tensor(-0.0566),\n"," 'pixel625': tensor(0.0070),\n"," 'pixel626': tensor(-0.2852),\n"," 'pixel627': tensor(0.3066),\n"," 'pixel628': tensor(-0.0937),\n"," 'pixel629': tensor(-0.4654),\n"," 'pixel630': tensor(0.1764),\n"," 'pixel631': tensor(-0.2268),\n"," 'pixel632': tensor(0.1962),\n"," 'pixel633': tensor(0.0383),\n"," 'pixel634': tensor(-0.1998),\n"," 'pixel635': tensor(0.3052),\n"," 'pixel636': tensor(-0.3078),\n"," 'pixel637': tensor(-0.3823),\n"," 'pixel638': tensor(-0.0649),\n"," 'pixel639': tensor(-0.3106),\n"," 'pixel640': tensor(-0.0049),\n"," 'pixel641': tensor(-0.3071),\n"," 'pixel642': tensor(0.4170),\n"," 'pixel643': tensor(-0.3776),\n"," 'pixel644': tensor(-0.1271),\n"," 'pixel645': tensor(0.1943),\n"," 'pixel646': tensor(-0.4007),\n"," 'pixel647': tensor(0.2550),\n"," 'pixel648': tensor(0.4439),\n"," 'pixel649': tensor(0.4586),\n"," 'pixel650': tensor(-0.3888),\n"," 'pixel651': tensor(-0.2680),\n"," 'pixel652': tensor(0.4104),\n"," 'pixel653': tensor(0.0826),\n"," 'pixel654': tensor(0.1792),\n"," 'pixel655': tensor(0.2380),\n"," 'pixel656': tensor(0.1630),\n"," 'pixel657': tensor(0.0811),\n"," 'pixel658': tensor(0.0160),\n"," 'pixel659': tensor(0.2617),\n"," 'pixel660': tensor(-0.4495),\n"," 'pixel661': tensor(0.1315),\n"," 'pixel662': tensor(0.1844),\n"," 'pixel663': tensor(-0.1778),\n"," 'pixel664': tensor(0.1368),\n"," 'pixel665': tensor(-0.1116),\n"," 'pixel666': tensor(-0.0913),\n"," 'pixel667': tensor(-0.3836),\n"," 'pixel668': tensor(-0.4651),\n"," 'pixel669': tensor(-0.1722),\n"," 'pixel670': tensor(0.3458),\n"," 'pixel671': tensor(0.0519),\n"," 'pixel672': tensor(-0.3962),\n"," 'pixel673': tensor(0.0602),\n"," 'pixel674': tensor(-0.1812),\n"," 'pixel675': tensor(-0.0735),\n"," 'pixel676': tensor(-0.3769),\n"," 'pixel677': tensor(0.2051),\n"," 'pixel678': tensor(0.3554),\n"," 'pixel679': tensor(-0.3396),\n"," 'pixel680': tensor(-0.2066),\n"," 'pixel681': tensor(-0.2077),\n"," 'pixel682': tensor(-0.0116),\n"," 'pixel683': tensor(0.1657),\n"," 'pixel684': tensor(0.4612),\n"," 'pixel685': tensor(0.0516),\n"," 'pixel686': tensor(0.4512),\n"," 'pixel687': tensor(-0.1411),\n"," 'pixel688': tensor(0.2451),\n"," 'pixel689': tensor(-0.3395),\n"," 'pixel690': tensor(0.2978),\n"," 'pixel691': tensor(0.4671),\n"," 'pixel692': tensor(-0.1628),\n"," 'pixel693': tensor(-0.1895),\n"," 'pixel694': tensor(0.1665),\n"," 'pixel695': tensor(-0.2850),\n"," 'pixel696': tensor(-0.0055),\n"," 'pixel697': tensor(-0.2126),\n"," 'pixel698': tensor(-0.3808),\n"," 'pixel699': tensor(-0.2650),\n"," 'pixel700': tensor(0.2225),\n"," 'pixel701': tensor(-0.3797),\n"," 'pixel702': tensor(0.1352),\n"," 'pixel703': tensor(0.2077),\n"," 'pixel704': tensor(0.3332),\n"," 'pixel705': tensor(0.3473),\n"," 'pixel706': tensor(0.2233),\n"," 'pixel707': tensor(-0.4095),\n"," 'pixel708': tensor(-0.1943),\n"," 'pixel709': tensor(0.0376),\n"," 'pixel710': tensor(0.4434),\n"," 'pixel711': tensor(-0.3095),\n"," 'pixel712': tensor(0.1875),\n"," 'pixel713': tensor(-0.3380),\n"," 'pixel714': tensor(0.5287),\n"," 'pixel715': tensor(-0.4239),\n"," 'pixel716': tensor(0.2305),\n"," 'pixel717': tensor(0.3217),\n"," 'pixel718': tensor(-0.1207),\n"," 'pixel719': tensor(-0.1975),\n"," 'pixel720': tensor(-0.2237),\n"," 'pixel721': tensor(0.0053),\n"," 'pixel722': tensor(-0.4572),\n"," 'pixel723': tensor(0.3880),\n"," 'pixel724': tensor(0.1039),\n"," 'pixel725': tensor(-0.0865),\n"," 'pixel726': tensor(0.0628),\n"," 'pixel727': tensor(-0.4212),\n"," 'pixel728': tensor(-0.3595),\n"," 'pixel729': tensor(-0.0070),\n"," 'pixel730': tensor(-0.2864),\n"," 'pixel731': tensor(0.0273),\n"," 'pixel732': tensor(0.2377),\n"," 'pixel733': tensor(-0.0290),\n"," 'pixel734': tensor(0.0466),\n"," 'pixel735': tensor(0.2953),\n"," 'pixel736': tensor(0.3252),\n"," 'pixel737': tensor(-0.4011),\n"," 'pixel738': tensor(0.1956),\n"," 'pixel739': tensor(-0.1286),\n"," 'pixel740': tensor(-0.2617),\n"," 'pixel741': tensor(-0.2896),\n"," 'pixel742': tensor(0.2777),\n"," 'pixel743': tensor(-0.0488),\n"," 'pixel744': tensor(-0.4086),\n"," 'pixel745': tensor(0.5063),\n"," 'pixel746': tensor(0.3795),\n"," 'pixel747': tensor(0.3448),\n"," 'pixel748': tensor(0.3117),\n"," 'pixel749': tensor(-0.4851),\n"," 'pixel750': tensor(-0.4841),\n"," 'pixel751': tensor(0.2263),\n"," 'pixel752': tensor(0.2172),\n"," 'pixel753': tensor(0.0325),\n"," 'pixel754': tensor(-0.0769),\n"," 'pixel755': tensor(-0.1767),\n"," 'pixel756': tensor(-0.1453),\n"," 'pixel757': tensor(0.3159),\n"," 'pixel758': tensor(-0.0257),\n"," 'pixel759': tensor(0.1365),\n"," 'pixel760': tensor(-0.1752),\n"," 'pixel761': tensor(-0.0453),\n"," 'pixel762': tensor(0.1941),\n"," 'pixel763': tensor(0.2265),\n"," 'pixel764': tensor(0.3782),\n"," 'pixel765': tensor(0.2537),\n"," 'pixel766': tensor(0.0720),\n"," 'pixel767': tensor(-0.2563),\n"," 'pixel768': tensor(0.1556),\n"," 'pixel769': tensor(0.0805),\n"," 'pixel770': tensor(0.4445),\n"," 'pixel771': tensor(0.1547),\n"," 'pixel772': tensor(-0.0905),\n"," 'pixel773': tensor(-0.3277),\n"," 'pixel774': tensor(0.3017),\n"," 'pixel775': tensor(0.4705),\n"," 'pixel776': tensor(0.1234),\n"," 'pixel777': tensor(-0.0052),\n"," 'pixel778': tensor(-0.2684),\n"," 'pixel779': tensor(-0.4286),\n"," 'pixel780': tensor(-0.1200),\n"," 'pixel781': tensor(0.0068),\n"," 'pixel782': tensor(0.3211),\n"," 'pixel783': tensor(-0.3081)}"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["show_coeffs()"]},{"cell_type":"markdown","metadata":{},"source":["These coefficients seem reasonable -- in general, older people and males were less likely to survive, and first class passengers were more likely to survive."]},{"cell_type":"markdown","metadata":{},"source":["## Submitting to Kaggle"]},{"cell_type":"markdown","metadata":{},"source":["Now that we've got a trained model, we can prepare a submission to Kaggle. To do that, first we need to read the test set:"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:35.172909Z","iopub.status.busy":"2022-05-30T22:38:35.172343Z","iopub.status.idle":"2022-05-30T22:38:35.188597Z","shell.execute_reply":"2022-05-30T22:38:35.187826Z","shell.execute_reply.started":"2022-05-30T22:38:35.172873Z"},"trusted":true},"outputs":[],"source":["tst_df = pd.read_csv(path/'test.csv')"]},{"cell_type":"markdown","metadata":{},"source":["In this case, it turns out that the test set is missing `Fare` for one passenger. We'll just fill it with `0` to avoid problems:"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:36.353392Z","iopub.status.busy":"2022-05-30T22:38:36.352687Z","iopub.status.idle":"2022-05-30T22:38:36.358237Z","shell.execute_reply":"2022-05-30T22:38:36.35739Z","shell.execute_reply.started":"2022-05-30T22:38:36.353355Z"},"trusted":true},"outputs":[],"source":["tst_df['Fare'] = tst_df.Fare.fillna(0)"]},{"cell_type":"markdown","metadata":{},"source":["Now we can just copy the same steps we did to our training set and do the same exact things on our test set to preprocess the data:"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:38.198549Z","iopub.status.busy":"2022-05-30T22:38:38.198257Z","iopub.status.idle":"2022-05-30T22:38:38.220592Z","shell.execute_reply":"2022-05-30T22:38:38.219629Z","shell.execute_reply.started":"2022-05-30T22:38:38.198519Z"},"trusted":true},"outputs":[],"source":["tst_df.fillna(modes, inplace=True)\n","tst_df['LogFare'] = np.log(tst_df['Fare']+1)\n","tst_df = pd.get_dummies(tst_df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\n","\n","\n","# Convert all boolean columns to integers\n","tst_df = tst_df.astype({col: int for col in tst_df.select_dtypes(include='bool').columns})\n","\n","tst_indep = tensor(tst_df[indep_cols].values, dtype=torch.float)\n","tst_indep = tst_indep / vals"]},{"cell_type":"markdown","metadata":{},"source":["Let's calculate our predictions of which passengers survived in the test set:"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:39.206547Z","iopub.status.busy":"2022-05-30T22:38:39.206216Z","iopub.status.idle":"2022-05-30T22:38:39.212386Z","shell.execute_reply":"2022-05-30T22:38:39.211631Z","shell.execute_reply.started":"2022-05-30T22:38:39.206512Z"},"trusted":true},"outputs":[],"source":["tst_df['Survived'] = (calc_preds(tst_indep, coeffs)>0.5).int()"]},{"cell_type":"markdown","metadata":{},"source":["The sample submission on the Kaggle competition site shows that we're expected to upload a CSV with just `PassengerId` and `Survived`, so let's create that and save it:"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:40.161382Z","iopub.status.busy":"2022-05-30T22:38:40.161034Z","iopub.status.idle":"2022-05-30T22:38:40.173242Z","shell.execute_reply":"2022-05-30T22:38:40.17258Z","shell.execute_reply.started":"2022-05-30T22:38:40.161336Z"},"trusted":true},"outputs":[],"source":["sub_df = tst_df[['PassengerId','Survived']]\n","sub_df.to_csv('sub.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["We can check the first few rows of the file to make sure it looks reasonable:"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:42.869402Z","iopub.status.busy":"2022-05-30T22:38:42.86855Z","iopub.status.idle":"2022-05-30T22:38:43.638832Z","shell.execute_reply":"2022-05-30T22:38:43.637559Z","shell.execute_reply.started":"2022-05-30T22:38:42.869362Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PassengerId,Survived\n","892,0\n","893,0\n","894,0\n","895,0\n","896,0\n","897,0\n","898,1\n","899,0\n","900,1\n"]}],"source":["!head sub.csv"]},{"cell_type":"markdown","metadata":{},"source":["When you click \"save version\" in Kaggle, and wait for the notebook to run, you'll see that `sub.csv` appears in the \"Data\" tab. Clicking on that file will show a *Submit* button, which allows you to submit to the competition."]},{"cell_type":"markdown","metadata":{},"source":["## Using matrix product"]},{"cell_type":"markdown","metadata":{},"source":["We can make things quite a bit neater...\n","\n","Take a look at the inner-most calculation we're doing to get the predictions:"]},{"cell_type":"code","execution_count":109,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:48.248084Z","iopub.status.busy":"2022-05-30T22:38:48.247768Z","iopub.status.idle":"2022-05-30T22:38:48.258935Z","shell.execute_reply":"2022-05-30T22:38:48.258184Z","shell.execute_reply.started":"2022-05-30T22:38:48.248052Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([-0.0108, -5.6278,  0.2997,  0.1003,  1.1623,  1.6804, -1.8950,  ..., -5.1155,  2.0699,  1.4349, -2.9620,  4.0186,  1.6766,\n","        -3.5795])"]},"execution_count":109,"metadata":{},"output_type":"execute_result"}],"source":["(val_indep*coeffs).sum(axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["Multiplying elements together and then adding across rows is identical to doing a matrix-vector product! Python uses the `@` operator to indicate matrix products, and is supported by PyTorch tensors. Therefore, we can replicate the above calculate more simply like so:"]},{"cell_type":"code","execution_count":110,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:51.959798Z","iopub.status.busy":"2022-05-30T22:38:51.959362Z","iopub.status.idle":"2022-05-30T22:38:51.97614Z","shell.execute_reply":"2022-05-30T22:38:51.975461Z","shell.execute_reply.started":"2022-05-30T22:38:51.959765Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([-0.0108, -5.6278,  0.2997,  0.1003,  1.1623,  1.6804, -1.8950,  ..., -5.1155,  2.0699,  1.4349, -2.9620,  4.0186,  1.6766,\n","        -3.5795])"]},"execution_count":110,"metadata":{},"output_type":"execute_result"}],"source":["val_indep@coeffs"]},{"cell_type":"markdown","metadata":{},"source":["It also turns out that this is much faster, because matrix products in PyTorch are very highly optimised.\n","\n","Let's use this to replace how `calc_preds` works:"]},{"cell_type":"code","execution_count":111,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:56.322255Z","iopub.status.busy":"2022-05-30T22:38:56.321807Z","iopub.status.idle":"2022-05-30T22:38:56.326812Z","shell.execute_reply":"2022-05-30T22:38:56.32606Z","shell.execute_reply.started":"2022-05-30T22:38:56.322213Z"},"trusted":true},"outputs":[],"source":["def calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs)*9"]},{"cell_type":"markdown","metadata":{},"source":["In order to do matrix-matrix products (which we'll need in the next section), we need to turn `coeffs` into a column vector (i.e. a matrix with a single column), which we can do by passing a second argument `1` to `torch.rand()`, indicating that we want our coefficients to have one column:"]},{"cell_type":"code","execution_count":112,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:58.895779Z","iopub.status.busy":"2022-05-30T22:38:58.895467Z","iopub.status.idle":"2022-05-30T22:38:58.900851Z","shell.execute_reply":"2022-05-30T22:38:58.899931Z","shell.execute_reply.started":"2022-05-30T22:38:58.895744Z"},"trusted":true},"outputs":[],"source":["def init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_()"]},{"cell_type":"markdown","metadata":{},"source":["We'll also need to turn our dependent variable into a column vector, which we can do by indexing the column dimension with the special value `None`, which tells PyTorch to add a new dimension in this position:"]},{"cell_type":"code","execution_count":113,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:38:59.789678Z","iopub.status.busy":"2022-05-30T22:38:59.788799Z","iopub.status.idle":"2022-05-30T22:38:59.794227Z","shell.execute_reply":"2022-05-30T22:38:59.793326Z","shell.execute_reply.started":"2022-05-30T22:38:59.789625Z"},"trusted":true},"outputs":[],"source":["trn_dep = trn_dep[:,None]\n","val_dep = val_dep[:,None]"]},{"cell_type":"markdown","metadata":{},"source":["We can now train our model as before and confirm we get identical outputs...:"]},{"cell_type":"code","execution_count":121,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:39:38.071003Z","iopub.status.busy":"2022-05-30T22:39:38.070545Z","iopub.status.idle":"2022-05-30T22:39:38.094666Z","shell.execute_reply":"2022-05-30T22:39:38.093641Z","shell.execute_reply.started":"2022-05-30T22:39:38.070972Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["4.426; 4.395; 4.345; 4.255; 4.068; 3.621; 2.868; 2.633; 2.723; 2.819; 2.844; 2.526; 2.432; 2.553; 2.101; 2.244; 1.969; 2.156; 1.973; 2.209; 1.953; 2.124; 1.924; 2.067; 1.923; 2.047; 1.922; 2.013; 1.904; 1.971; "]}],"source":["coeffs = train_model(lr=0.1)"]},{"cell_type":"markdown","metadata":{},"source":["...and identical accuracy:"]},{"cell_type":"code","execution_count":122,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:39:40.463735Z","iopub.status.busy":"2022-05-30T22:39:40.463301Z","iopub.status.idle":"2022-05-30T22:39:40.469684Z","shell.execute_reply":"2022-05-30T22:39:40.468652Z","shell.execute_reply.started":"2022-05-30T22:39:40.463702Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor(0.1827)"]},"execution_count":122,"metadata":{},"output_type":"execute_result"}],"source":["acc(coeffs)"]},{"cell_type":"markdown","metadata":{},"source":["## A neural network"]},{"cell_type":"markdown","metadata":{},"source":["We've now got what we need to implement our neural network.\n","\n","First, we'll need to create coefficients for each of our layers. Our first set of coefficients will take our `n_coeff` inputs, and create `n_hidden` outputs. We can choose whatever `n_hidden` we like -- a higher number gives our network more flexibility, but makes it slower and harder to train. So we need a matrix of size `n_coeff` by `n_hidden`. We'll divide these coefficients by `n_hidden` so that when we sum them up in the next layer we'll end up with similar magnitude numbers to what we started with.\n","\n","Then our second layer will need to take the `n_hidden` inputs and create a single output, so that means we need a `n_hidden` by `1` matrix there. The second layer will also need a constant term added."]},{"cell_type":"code","execution_count":123,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:39:44.428599Z","iopub.status.busy":"2022-05-30T22:39:44.428254Z","iopub.status.idle":"2022-05-30T22:39:44.434009Z","shell.execute_reply":"2022-05-30T22:39:44.433164Z","shell.execute_reply.started":"2022-05-30T22:39:44.428563Z"},"trusted":true},"outputs":[],"source":["def init_coeffs(n_hidden=20):\n","    layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden\n","    layer2 = torch.rand(n_hidden, 1)-0.3\n","    const = torch.rand(1)[0]\n","    return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()"]},{"cell_type":"markdown","metadata":{},"source":["Now we have our coefficients, we can create our neural net. The key steps are the two matrix products, `indeps@l1` and `res@l2` (where `res` is the output of the first layer). The first layer output is passed to `F.relu` (that's our non-linearity), and the second is passed to `torch.sigmoid` as before."]},{"cell_type":"code","execution_count":124,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:39:45.302903Z","iopub.status.busy":"2022-05-30T22:39:45.302573Z","iopub.status.idle":"2022-05-30T22:39:45.309472Z","shell.execute_reply":"2022-05-30T22:39:45.308498Z","shell.execute_reply.started":"2022-05-30T22:39:45.302864Z"},"trusted":true},"outputs":[],"source":["import torch.nn.functional as F\n","\n","def calc_preds(coeffs, indeps):\n","    l1,l2,const = coeffs\n","    res = F.relu(indeps@l1)\n","    res = res@l2 + const\n","    return torch.sigmoid(res)"]},{"cell_type":"markdown","metadata":{},"source":["Finally, now that we have more than one set of coefficients, we need to add a loop to update each one:"]},{"cell_type":"code","execution_count":125,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:39:55.366945Z","iopub.status.busy":"2022-05-30T22:39:55.3665Z","iopub.status.idle":"2022-05-30T22:39:55.371578Z","shell.execute_reply":"2022-05-30T22:39:55.370699Z","shell.execute_reply.started":"2022-05-30T22:39:55.366914Z"},"trusted":true},"outputs":[],"source":["def update_coeffs(coeffs, lr):\n","    for layer in coeffs:\n","        layer.sub_(layer.grad * lr)\n","        layer.grad.zero_()"]},{"cell_type":"markdown","metadata":{},"source":["That's it -- we're now ready to train our model!"]},{"cell_type":"code","execution_count":127,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:39:58.189982Z","iopub.status.busy":"2022-05-30T22:39:58.189651Z","iopub.status.idle":"2022-05-30T22:39:58.227202Z","shell.execute_reply":"2022-05-30T22:39:58.226226Z","shell.execute_reply.started":"2022-05-30T22:39:58.189951Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["3.868; 3.801; 3.746; 3.716; 3.698; 3.687; 3.679; 3.672; 3.668; 3.664; 3.662; 3.659; 3.658; 3.656; 3.655; 3.654; 3.653; 3.652; 3.651; 3.651; 3.650; 3.649; 3.649; 3.649; 3.648; 3.648; 3.647; 3.647; 3.647; 3.647; "]}],"source":["coeffs = train_model(lr=0.1)"]},{"cell_type":"code","execution_count":128,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:40:16.338016Z","iopub.status.busy":"2022-05-30T22:40:16.337512Z","iopub.status.idle":"2022-05-30T22:40:16.368327Z","shell.execute_reply":"2022-05-30T22:40:16.367439Z","shell.execute_reply.started":"2022-05-30T22:40:16.337959Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["3.868; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; "]}],"source":["coeffs = train_model(lr=20)"]},{"cell_type":"markdown","metadata":{},"source":["It's looking good -- our loss is lower than before. Let's see if that translates to a better result on the validation set:"]},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:40:18.644458Z","iopub.status.busy":"2022-05-30T22:40:18.644153Z","iopub.status.idle":"2022-05-30T22:40:18.651372Z","shell.execute_reply":"2022-05-30T22:40:18.650102Z","shell.execute_reply.started":"2022-05-30T22:40:18.644427Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor(0.1113)"]},"execution_count":129,"metadata":{},"output_type":"execute_result"}],"source":["acc(coeffs)"]},{"cell_type":"markdown","metadata":{},"source":["In this case our neural net isn't showing better results than the linear model. That's not surprising; this dataset is very small and very simple, and isn't the kind of thing we'd expect to see neural networks excel at. Furthermore, our validation set is too small to reliably see much accuracy difference. But the key thing is that we now know exactly what a real neural net looks like!"]},{"cell_type":"markdown","metadata":{},"source":["## Deep learning"]},{"cell_type":"markdown","metadata":{},"source":["The neural net in the previous section only uses one hidden layer, so it doesn't count as \"deep\" learning. But we can use the exact same technique to make our neural net deep, by adding more matrix multiplications.\n","\n","First, we'll need to create additional coefficients for each layer:"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:40:55.255291Z","iopub.status.busy":"2022-05-30T22:40:55.25457Z","iopub.status.idle":"2022-05-30T22:40:55.261806Z","shell.execute_reply":"2022-05-30T22:40:55.261271Z","shell.execute_reply.started":"2022-05-30T22:40:55.255242Z"},"trusted":true},"outputs":[],"source":["def init_coeffs():\n","    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n","    sizes = [n_coeff] + hiddens + [1]\n","    n = len(sizes)\n","    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n","    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n","    for l in layers+consts: l.requires_grad_()\n","    return layers,consts"]},{"cell_type":"markdown","metadata":{},"source":["You'll notice here that there's a lot of messy constants to get the random numbers in just the right ranges. When you train the model in a moment, you'll see that the tiniest changes to these initialisations can cause our model to fail to train at all! This is a key reason that deep learning failed to make much progress in the early days -- it's very finicky to get a good starting point for our coefficients. Nowadays, we have ways to deal with that, which we'll learn about in other notebooks.\n","\n","Our deep learning `calc_preds` looks much the same as before, but now we loop through each layer, instead of listing them separately:"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:40:57.610974Z","iopub.status.busy":"2022-05-30T22:40:57.610142Z","iopub.status.idle":"2022-05-30T22:40:57.618154Z","shell.execute_reply":"2022-05-30T22:40:57.617329Z","shell.execute_reply.started":"2022-05-30T22:40:57.610916Z"},"trusted":true},"outputs":[],"source":["import torch.nn.functional as F\n","\n","def calc_preds(coeffs, indeps):\n","    layers,consts = coeffs\n","    n = len(layers)\n","    res = indeps\n","    for i,l in enumerate(layers):\n","        res = res@l + consts[i]\n","        if i!=n-1: res = F.relu(res)\n","    return torch.sigmoid(res)"]},{"cell_type":"markdown","metadata":{},"source":["We also need a minor update to `update_coeffs` since we've got `layers` and `consts` separated now:"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:41:08.513049Z","iopub.status.busy":"2022-05-30T22:41:08.512494Z","iopub.status.idle":"2022-05-30T22:41:08.519219Z","shell.execute_reply":"2022-05-30T22:41:08.518093Z","shell.execute_reply.started":"2022-05-30T22:41:08.512999Z"},"trusted":true},"outputs":[],"source":["def update_coeffs(coeffs, lr):\n","    layers,consts = coeffs\n","    for layer in layers+consts:\n","        layer.sub_(layer.grad * lr)\n","        layer.grad.zero_()"]},{"cell_type":"markdown","metadata":{},"source":["Let's train our model..."]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:41:23.633004Z","iopub.status.busy":"2022-05-30T22:41:23.632516Z","iopub.status.idle":"2022-05-30T22:41:23.666981Z","shell.execute_reply":"2022-05-30T22:41:23.666048Z","shell.execute_reply.started":"2022-05-30T22:41:23.632953Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; 3.639; "]}],"source":["coeffs = train_model(lr=4)"]},{"cell_type":"markdown","metadata":{},"source":["...and check its accuracy:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-05-30T22:41:25.491182Z","iopub.status.busy":"2022-05-30T22:41:25.490656Z","iopub.status.idle":"2022-05-30T22:41:25.497888Z","shell.execute_reply":"2022-05-30T22:41:25.49695Z","shell.execute_reply.started":"2022-05-30T22:41:25.491146Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor(0.9007)"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["acc(coeffs)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["tst_indep = pd.read_csv(path/'test.csv')"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"too many values to unpack (expected 2)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tst_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[43mcalc_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtst_indep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoeffs\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mint()\n","Cell \u001b[0;32mIn[44], line 4\u001b[0m, in \u001b[0;36mcalc_preds\u001b[0;34m(coeffs, indeps)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_preds\u001b[39m(coeffs, indeps):\n\u001b[0;32m----> 4\u001b[0m     layers,consts \u001b[38;5;241m=\u001b[39m coeffs\n\u001b[1;32m      5\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(layers)\n\u001b[1;32m      6\u001b[0m     res \u001b[38;5;241m=\u001b[39m indeps\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}],"source":["tst_df['label'] = (calc_preds(tst_indep, coeffs)).int()"]},{"cell_type":"markdown","metadata":{},"source":["## Final thoughts"]},{"cell_type":"markdown","metadata":{},"source":["It's actually pretty cool that we've managed to create a real deep learning model from scratch and trained it to get over 80% accuracy on this task, all in the course of a single notebook!\n","\n","The \"real\" deep learning models that are used in research and industry look very similar to this, and in fact if you look inside the source code of any deep learning model you'll recognise the basic steps are the same.\n","\n","The biggest differences in practical models to what we have above are:\n","\n","- How initialisation and normalisation is done to ensure the model trains correctly every time\n","- Regularization (to avoid over-fitting)\n","- Modifying the neural net itself to take advantage of knowledge of the problem domain\n","- Doing gradient descent steps on smaller batches, rather than the whole dataset.\n","\n","I'll be adding notebooks about all these later, and will add links here once they're ready.\n","\n","If you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. (BTW, be sure you're looking at my [original notebook here](https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch) when you do that, and are not on your own copy of it, otherwise your upvote won't get counted!) And if you have any questions or comments, please pop them below -- I read every comment I receive!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":26502,"sourceId":3136,"sourceType":"competition"}],"dockerImageVersionId":30184,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
